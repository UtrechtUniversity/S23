[
  {
    "objectID": "day1_exercises.html",
    "href": "day1_exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Measurement invariance is important if we want to compare individuals from different groups, or the means of different groups. Measurement invariance implies that two individuals from different groups who have the same latent score, have the same expected observed scores. It also means that if there are observed mean differences between two groups, these can only stem from latent mean differences.\nWhen measurement invariance does not hold, we call the test (i.e., measurement instrument) biased. We may then look for the source of the bias, and try to account for this in our analyses.\nHere we will show how to specify a sequence of models to test whether measurement invariance holds. These steps include:\n\nconfigural invariance; this implies the same model in each group without any constraints across the groups\nweak factorial invariance; this implies the factor loadings are constrained across the groups to be the same\nstrong factorial invariance; this implies the factor loadings and the intercepts are constrained to be the same across the groups, while the latent means in the second (and subsequent) group(s) are allowed to be estimated freely.\n\nWe start with drawing the model as a path diagram. The data come from Sabatelli and Bartle-Haring (2003). They obtained measures from 103 married couples regarding their marital adjustment and their family of origin. Here we analyze these data using a multiple group approach, although the individuals in the two groups are not independent of each other, and there are other approaches that would be more appropriate for this (such as described in the original paper, where the cases are couples, and the data of each couple consists of variables measured in the husband and other variables are measured in the wives).\nThe variables included are:\n\nSatisfaction (S): higher levels imply fewer complaints\nIntimacy (I): higher scores imply more self-disclosures, experiences of empathy and affection, and feelings of emotional closeness toward the marital partner\nFather (F): higher scores imply a better relationship between the participant and his/her father\nMother (M): higher scores imply a better relationship between the participant and his/her mother\nFather-mother (FM): higher scores a better relationship between the parents of the participant\n\nThe idea is that the first two variables measure Marital Adjustment (MA) and the latter three variables measure the quality of relationships in the Family of Origin (FO). Furthermore, it is assumed that FO is a predictor of MA.\n\n\nDraw the path diagram for this structural equation model.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nMeasurement part of the model: S and I are indicators of MA; F, M, and FM are indicators of FO. Structural part of the model: MA is regressed on FO.\n\n\n\n\nBefore doing the actual model that includes the structural relation between the latent variables, we start with only the measurement model; this means that we specify a two-factor model in which the latent variables are allowed to covary (i.e., a two-headed arrow between MA and FO). We use this model to investigate whether strong factorial invariance holds by testing whether the assumptions of this model hold.\n\n\n\nThe data for this exercise are included in the file named Family.dat; note it only contains the summary statistics, that is, it contains the means, standard deviations, and correlation matrices for each group (men first, then women), that were printed in the original paper. Have a look at these.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYou can open the data either in Mplus, or with a program like notepad. This is what you see:\n161.779 138.382 86.229 86.392 85.046\n32.936 22.749 13.390 13.679 14.382\n1\n.740 1\n.265 .422 1\n.305 .401 .791 1\n.315 .351 .662 .587 1\n155.547 137.971 82.764 85.494 81.003\n31.168 20.094 11.229 11.743 13.220\n1\n.658 1\n.288 .398 1\n.171 .295 .480 1\n.264 .305 .554 .422 1\n\n\n\n\n\n\nTypically when you have data, you will have the raw data. In that case you will have the observed variables and a grouping variable in one file. See example 5.14 in the Mplus Users Guide for how to specify the DATA and VARIABLE commands in that case.\nHere we only have the summary data. To specify the DATA and VARIABLE commands in this case, use:\nDATA:   NGROUPS ARE 2;\n        TYPE IS MEANS STD CORR;\n        FILE IS Family.dat;\n        NOBSERVATIONS ARE 103 103;\n\nVARIABLE:   NAMES ARE S I F M FM;\nWe will begin with using the automatic option in Mplus that allows us to run all three models that are needed to investigate measurement invariance. This can be done by adding:\nANALYSIS:   MODEL = CONFIGURAL METRIC SCALAR;\nwhere CONFIGURAL is the model without any constraints across the groups; METRIC is the model we refer to as weak factorial invariance, that is, equal factor loadings across groups; and SCALAR is what we refer to as strong factorial invariance, that is, equal factor loadings and intercepts across groups, and freely estimated latent means in group 2 (and subsequent groups, if there are any).\nUse the code above, and add:\n\nTITLE command\nMODEL command, in which you just specify the general factor model (so no need to specify separate models for separate groups!)\nOUTPUT command\n\nRun the model and check the output. Describe what you see.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIn the top menu in Mplus you can change the format of the output file by going to Mplus and choosing HTML output, and then run the model. This will result in output with links that makes it easier to navigate through it. At the beginning of the output, you get to see:\nOUTPUT SECTIONS\n\nInput Instructions \nInput Warnings And Errors \nSummary Of Analysis \nSample Statistics \nModel Fit Information \n- For The Configural Model \n- For The Metric Model \n- For The Scalar Model \nModel Results \n- For The Configural Model \n- For The Metric Model \n- For The Scalar Model \nTechnical 1 Output \n- For The Configural Model \n- For The Metric Model \n- For The Scalar Model \nTechnical 9 Output \nHence, we see that for each of the three models we get information about model fit, we get the parameter estimates, and the additional output (here, we asked for TECH1). We also get TECH9 which contains the warnings for the three models.\n\n\n\nWhile this automatic option is of course very convenient in practice, we will now focus in this exercise on how to specify and run these three models ourselves. The point of this is that we see how we can overrule defaults in Mplus, and we consider the models one by one (and check the TECH1 output), and consider alternative ways of scaling these models.\n\n\n\n\n\nIf we specify the model for configural invariance:\n\nhow many sample statistics are there?\nhow many free parameters are there?\nhow many df are there?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nSample statistics. We have 5 observed variables in each group. Hence, we have 5*6/2 = 15 unique elements in the covariance matrix S for each group, plus 5 observed means, so 20 sample statistics per group. That makes 40 sample statistics in total.\nFree parameter. In each group we estimate:\n\n3 factor loadings (there are 5 factor loadings, but 2 are used for scaling)\n2 factor variances\n1 factor covariance\n5 residual variances\n5 intercepts\n\nThat makes 16 parameters per group; hence, 32 free parameters in total.\nDegrees of freedom. The degrees of freedom are therefore: \\(df = 40 - 32 = 8\\).\n\n\n\n\n\n\nSpecify the two-factor model for configural invariance.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nMODEL:      MA BY S I;\n            FO BY F M FM; \n\nMODEL g2:   MA BY S@1 I;\n            FO BY F@1 M FM;\n            [S I F M FM];\n            [MA@0 FO@0];\nwhere the first MODEL: command is the general model specification, and the second part MODEL g2; is overruling the multiple group factor analysis defaults through:\n\nfreeing the factor loadings (so they are not identical to the factor loadings in g1), and using @1 to ensure the factors are scaled\nfreeing the intercepts in the second group (so they are not identical to the intercepts in g1)\nfixing the latent means to zero (to ensure identification)\n\nAlternatively, one could also say:\nMODEL:      MA BY S I;\n            FO BY F M FM; \n\nMODEL g2:   MA BY I;\n            FO BY M FM;\n            [S I F M FM];\n            [MA@0 FO@0];\nHere, we are doing the same thing, but not overruling the default for the factor loadings of the first indicator of each latent variable (hence the initial scaling remains in tact).\nIn addition to this MODEL command, we specify the OUTPUT command as:\nOUTPUT: TECH1 MOD(4);\n\n\n\n\n\n\nRun this model, and check whether the TECH1 output matches your answer under Question 4 regarding where the free parameters are. Also check the warning(s); is there a reason for concern?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe get this warning:\nTHE MODEL ESTIMATION TERMINATED NORMALLY\n\nWARNING:  THE RESIDUAL COVARIANCE MATRIX (THETA) IN GROUP G1 IS NOT\nPOSITIVE DEFINITE.  THIS COULD INDICATE A NEGATIVE VARIANCE/RESIDUAL\nVARIANCE FOR AN OBSERVED VARIABLE, A CORRELATION GREATER OR EQUAL TO ONE\nBETWEEN TWO OBSERVED VARIABLES, OR A LINEAR DEPENDENCY AMONG MORE THAN TWO\nOBSERVED VARIABLES.  CHECK THE RESULTS SECTION FOR MORE INFORMATION.\nPROBLEM INVOLVING VARIABLE I.\nThis implies that the residual covariance matrix contains a combination of numbers (i.e., parameter estimates) that are not possible in a covariance matrix. We can start with checking the relevant parameter estimates in the first group:\nResidual Variances\n  S                528.470    130.512      4.049      0.000\n  I                -39.883    109.198     -0.365      0.715\n  F                 21.613     10.983      1.968      0.049\n  M                 53.509     11.710      4.570      0.000\n  FM               103.074     16.168      6.375      0.000\nIt shows that the residual variance for the indicator I (intimacy) is negative; this is not possible (as variances are by definition zero or larger). This is referred to as a Heywood case, and is typically interpreted as meaning that the model is too complicated for the data (for instance because the model is really wrong, or because the sample size is too small). I is an indicator of MA, which only has one other indicator (i.e., S); this is often a difficult situation in terms of estimation.\nOne action we could take is setting this residual variance to zero (note the parameter is not significantly different from zero). That would mean this variable is a perfect indicator of the latent variable MA, as there would be no measurement error at all; this would in turn mean we only need this indicator, and there is no need for the second indicator to measure the latent variable.\nBut for now, we will just leave the variable in, and consider the model fit before moving on to testing weak factorial invariance.\n\n\n\n\n\n\nFor now, we ignore this warning. What is the model fit?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nChi-Square Test of Model Fit\n\n          Value                              6.025\n          Degrees of Freedom                     8    \n          P-Value                           0.6444\nThe chi-square test indicates the model fits well (note btw that the degrees of freedom are indeed 8). We see this chi-square is simply the sum of the chi-squares that are obtained for each group, as presented in the output:\nChi-Square Contribution From Each Group\n\n          G1                                 4.688\n          G2                                 1.337\nThe other fit measures also indicate the model fits very well:\nRMSEA (Root Mean Square Error Of Approximation)\n\n          Estimate                           0.000\n          90 Percent C.I.                    0.000  0.095\n          Probability RMSEA &lt;= .05           0.786\n\nCFI/TLI\n\n          CFI                                1.000\n          TLI                                1.000\n\nSRMR (Standardized Root Mean Square Residual)\n\n          Value                              0.019\n\n\n\n\n\n\n\n\n\nNext, specify the model for weak factorial invariance. How many df would this model have? Run this model and report on the model fit.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIn the model for weak factorial invariance, we set the factor loadings invariant across groups, while not constraining the mean structure. In practice, this implies we no longer have to overrule the defaults for the factor loadings, but we still need to overrule the defaults for the intercepts and latent means. Hence, the model specification becomes:\nMODEL:      MA BY S I;\n            FO BY F M FM; \n\nMODEL g2:   [S I F M FM];\n            [MA@0 FO@0];\nWhile in the previous model we were estimating 3 factor loadings in each group (so 6 factor loadings in total), we now estimate 3 factor loadings for both groups. Hence, this model has 3 free parameters less, and thus 3 df more than the model for configural invariance; the df should thus be 8+3=11. This also becomes clear when looking at the TECH1 output of this model (see the numbers that identify the free parameters in the matrix LAMBDA).\nWe see this in the chi-square test:\nChi-Square Test of Model Fit\n\n          Value                              8.735\n          Degrees of Freedom                    11\n          P-Value                           0.6463\nWe see indeed the number of df is 11.\n\n\n\n\n\n\nDo a chi-square difference test to determine whether the assumption of weak factorial invariance (i.e., equal factor loadings across groups) holds.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\(\\Delta \\chi^2 = 8.735 - 6.025 = 2.710\\)\n\\(\\Delta df = 11 - 8 = 3\\)\nHence, the chi-square difference is 2.710 with df=3. The p-value for this is 0.4385 (for an online chi-square calculator, see for instance: https://www.fourmilab.ch/rpkp/experiments/analysis/chiCalc.html).\nThis means that our H0 (i.e., weak factorial invariance) is not rejected. Put differently, the constraints for weak factorial invariance can be imposed.\n\n\n\n\n\n\n\n\n\nNext, specify the model for strong factorial invariance. Again, indicate how many df this model will have, run the model, and report on the model fit.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nStrong factorial invariance is the default multiple group factor model that Mplus runs. Hence, it does not require us to overrule any of its defaults. Compared to the previous model, this means that it will constrain all five intercepts to be identical across the two groups, but at the same time, it will freely estimate the two latent means in the second group. Hence, the difference in df is 5-2=3; the new model with have 3 df more than the one we had before.\nThe MODEL command for this model is very simple; we only need:\nMODEL:      MA BY S I;\n            FO BY F M FM; \nThe chi-square of this model is:\nChi-Square Test of Model Fit\n\n        Value                             15.647\n        Degrees of Freedom                    14\n        P-Value                           0.3354\nWe see that indeed the df are 3 more than before (\\(11 + 3 = 14\\)).\n\n\n\n\n\n\nDo a chi-square difference test to determine whether the assumption of strong factorial invariance (i.e., equal intercepts; only latent mean differences) holds.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe compare the current model fit to that of the previous model (i.e., the one for weak factorial invariance), to determine whether the additional constraints are tenable.\nHence we have:\n\\(\\Delta \\chi^2 = 15.647 - 8.735 = 6.912\\)\n\\(\\Delta df = 14 - 11 = 3\\)\nA chi-square difference test of 6.912 with 3 df has a p-value of 0.0748. Hence, again, the H0 is not rejected, which now means we can assume that strong factorial invariance holds.\n\n\n\n\n\n\nCheck the TECH1 output for the latter model. How do you see the constraints that were imposed?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYou can see the intercepts and factor loadings are constrained across the two groups, because the same numbers are used to show which ones are estimated freely:\n         NU\n            S             I             F             M             FM\n           ________      ________      ________      ________      ________\n                1             2             3             4             5\n\n\n         LAMBDA\n           MA            FO\n            ________      ________\n S                  0             0\n I                  6             0\n F                  0             0\n M                  0             7\n FM                 0             8\n\n\n\n\n\n\n\nAbove, we have used the usual way of scaling:\n\nfirst factor loading of each latent variable fixed to 1 (in each group)\nall latent means fixed to 0 (in each group), combined with estimating the intercepts for the observed variables freely (in each group)\n\nIn this case, the step from configural to weak factorial invariance now consists of only adding more constraints, which makes it easy to see these two models are nested (i.e., the model for weak factorial invariance is nested under the model for configural invariance).\nHowever, it is less obvious that the model for strong factorial invariance is nested under the model for weak factorial invariance, because it includes constraining parameters (i.e., the intercepts have to be equal across groups), but also requires freeing parameters (i.e., the latent means in the second group are freed).\nTo ensure that these models are nested, we use an alternative way of specifying these models, based on a different way of scaling. The key issue here is that these alternative model specification are simply reparameterizations of the the same models; they are equivalent models, as we can see because they lead to the exact same model fit.\n\n\nInstead of scaling with the latent means fixed to zero and the intercepts all estimated freely, scale the latent variable in the model for configural invariance through setting the intercept for the first indicator to zero, while allowing the latent mean to be estimated freely. Compare the model fit to the one obtained above for the configural invariance model.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe MODEL command can be specified as:\nMODEL:        MA BY S I;\n              FO BY F M FM;\n\nMODEL g1:     [S@0 F@0];\n              [MA FO];\n\nMODEL g2:     MA BY S@1 I;\n              FO BY F@1 M FM;\n              [S@0 I F@0 M FM];\nWhen running the model like this, it does not converge within the default number of iterations. We get this message:\nNO CONVERGENCE.  NUMBER OF ITERATIONS EXCEEDED.\nand there is no model fit, standard errors, or p-values. This can mean the model is too complicated. It can also mean you need to specify (better) starting values. As a first step, we can just increase the number of iterations, by adding:\nANALYSIS:   ITERATIONS ARE 20000;\nIn this case, this solves it, that is, the model now converges. When considering the model fit, we see it is exactly the same as that of the model discussed in Question 7.\nChi-Square Test of Model Fit\n\n          Value                              6.025\n          Degrees of Freedom                     8    \n          P-Value                           0.6444\nBecause the chi-square is exactly the same (and the df), this implies these models are statistically equivalent; they are identical, just parameterized differently. You cannot distinguish between them on statistical grounds.\nHence, this shows that this alternative way of scaling the latent variables gives a model that is equivalent.\n\n\n\n\n\n\nNext, use this alternative way of scaling and specify the model for weak factorial invariance.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe MODEL command can be specified as:\nMODEL:        MA BY S I;\n              FO BY F M FM;\n\nMODEL g1:     [S@0 F@0];\n              [MA FO];\n\nMODEL g2:     [S@0 I F@0 M FM];\nThat is, we no longer overrule the defaults for the factor loadings. Note that for this model to converge, we do not need to increase the number of iterations.\nWhen considering the model fit, we see it is exactly the same as that of the model discussed in Question 8:\nChi-Square Test of Model Fit\n\n          Value                              8.735\n          Degrees of Freedom                    11\n          P-Value                           0.6463\nThis means that the current model is statistically equivalent to the model we had before to impose weak factorial invariance.\n\n\n\n\n\n\nAnd now, the Moment Suprême: We will specify the model for strong factorial invariance using this alternative way of scaling. The key issue to notice here is that in going from weak factorial invariance to strong factorial invariance, we will now only add constraints, which makes it very clear that the latter model must be nested under the former.\nRemember that before, we were adding constraints (i.e., fixing the intercepts to be identical across the groups), but also were freeing parameters (i.e., allowing the latent means in the second group to be estimated freely); this makes it difficult to see that the model for strong factorial invariance is nested under that of weak factorial invariance.\nBut with this alternative way of scaling, which leads to statistically equivalant models (i.e., models with the exact fit, and which are thus indistinguishable), we now only add constraints on parameters when going from weak to strong factorial invariance, such that it is very obvious that (and how) they are nested.\nSpecify the model for strong factorial invariance using this alternative way of scaling.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe MODEL command can be specified as:\nMODEL:        MA BY S I;\n              FO BY F M FM;\n\nMODEL g1:     [S@0 F@0];\n              [MA FO];\n\nMODEL g2:     [S@0 F@0];\nThis model leads to the exactly same model fit as that of the model discussed in Question 10:\nChi-Square Test of Model Fit\n\n          Value                             15.647\n          Degrees of Freedom                    14\n          P-Value                           0.3354\nThis means that the current model is statistically equivalent to the model we had before to impose strong factorial invariance. This also means that we can now be certain that the model for strong factorial invariance is nested under the model for weak factorial invariance; this means we can do a chi-square difference test to compare them (as we already did above).\n\n\n\n\n\n\nWhile this alternative way of specifying the series of models is useful to seeing their nestedness, the first way of specifying the models also has advantages. One of these advantages is that it requires fewer defaults to be overruled. But more importantly, it allows us to easily determine in the strong factorial invariance model whether there are latent mean differences between the groups. Report and interpret these.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIn the first approach, the model for strong factorial invariance is based on constraining the factor loadings and intercepts across the groups, and freeing the latent means in the second group, while the means in the first group are fixed to zero.\nThat is, for G1 we get:\nMeans\n    MA                 0.000      0.000    999.000    999.000\n    FO                 0.000      0.000    999.000    999.000\nand for G2 we get:\nMeans\n    MA                -0.415      3.118     -0.133      0.894\n    FO                -3.140      1.655     -1.898      0.058\nThis implies that the significance test of the latter can be used to determine whether there are latent mean differences between the second and the first group. Here we get a p-value of 0.894 for MA, and a p-value of 0.058 for FO; hence for both we do not find evidence that they differ from zero, which means that for both we do not find evidence that the two groups differ.\n\n\n\n\n\n\n\nAs both the test for weak and for strong factorial invariance were non-significant, we can conclude that strong factorial invariance holds. This means we can compare individuals from these groups, and we can compare the means of these groups. It also means that the constructs “Marital Adjustment” and “Family of Origin” are measured in the same way in these two groups, and that we can specify a structural model for these latent variables in the two groups to investigate how they are related.\n\n\nSpecify the model as initially intended, with FO as a predictor of MA at the latent level, while assuming strong factorial invariance.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe model is now specified as:\nMODEL:      MA BY S I;\n            FO BY F M FM; \n            MA ON FO;\n\n\n\n\n\n\nRun this model, and compare the model fit to that of the model for strong factorial invariance that we had before. What do you conclude?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThese models have the exact same model fit (same chi-square and df). This means these models are statistically equivalent. Hence, it does not matter whether you estimate the covariance between the two latent variables, or a regression coefficient.\n\n\n\n\n\n\nWhen comparing the TECH1 output of these two models, what difference do you spot?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIn the two-factor model we have for G1:\n          BETA\n              MA            FO\n              ________      ________\n MA                 0             0\n FO                 0             0\n\n\n           PSI\n              MA            FO\n              ________      ________\n MA                14\n FO                15            16\nthat is, no structural parameters, and a covariance is Psi; in contrast in the regression model we have for G1:\n               BETA\n                  MA            FO\n                  ________      ________\n     MA                 0            14\n     FO                 0             0\n\n\n               PSI\n                  MA            FO\n                  ________      ________\n     MA                15\n     FO                 0            16\nshowing there is a structural parameter in Beta (going to MA, coming from FO), and there is no covariance estimated in Psi. The same is true for G2.\n\n\n\n\n\n\nAs a final step, we want to investigate whether the regression coefficient is different in the two groups or not. How can you investigate this?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIn the model above, the regression parameter was not constrained across the groups. We can now specify a model in which we constrain the parameter to be invariant across the two groups; that model will be nested under the previous model, and we can thus do a chi-square difference test to see whether the constraint holds.\nTo constrain the parameter across the groups, we only have to give it a label (i.e., a name). Here we will give it the label c (you could als give it a longer name, such as coeff). The model is now specified as:\n        MODEL:      MA BY S I;\n                    FO BY F M FM; \n                    MA ON FO (c);\nTo see that this leads to having the same regression coefficient in both groups, we can check the TECH1 output. For G1 we have (only showing part of it):\n           ALPHA\n              MA            FO\n              ________      ________\n                    0             0\n\n\n           BETA\n               MA            FO\n              ________      ________\n MA                 0            14\n FO                 0             0\n\n\n          PSI\n             MA            FO\n              ________      ________\n MA                15\n FO                 0            16\nand for G2:\n         ALPHA\n             MA            FO\n             ________      ________\n                  22            23\n\n\n          BETA\n              MA            FO\n              ________      ________\n MA                 0            14\n FO                 0             0\n\n\n           PSI\n              MA            FO\n              ________      ________\n MA                24\n FO                 0            25\nIt shows that the structural parameter in both cases is the same parameter (number 14).\nThe model fit is:\nChi-Square Test of Model Fit\n\n          Value                             16.315\n          Degrees of Freedom                    15\n          P-Value                           0.3614\nshowing it has 1 df more than the model we had before (because we estimate 1 parameter less). Doing a chi-square difference test we get:\n\\(\\Delta \\chi^2 = 16.315 - 15.647 = 0.688\\) with \\(df-1\\), which gives a p-value of 0.4137, which means the test is not significant. Hence, H0 does not have to be rejected, and we can therefore assume that the regression coefficient is identical across the two groups.\n\n\n\n\n\n\n\nTo summarize what we did in this exercise:\n\nwe were interested in investigating a full SEM model (with both a measurement model and a structural model), in two groups\nbefore we could investigate the structural part of the model (i.e., the latent regression), we had to determine whether there was measurement invariance across the groups\nto this end we ran a series of three models: configural invariance, weak factorial invariance and strong factorial invariance; these models are increasingly more restrictive, and we compared each subsequent model with the preceding one (using a chi-square different test) to see whether the additional constraints were tenable\nsince neither of the two chi-square difference test reached significance, we can conclude that strong factorial invariance holds; this means we are measuring the same constructs in both groups, and we can compare their latent means\nwe also considered an alternative way to scale the models (based on setting one of the intercepts per factor to zero, rather than the latent means), which more clearly shows that the model for strong factorial invariance is a special case of the model for weak factorial invariance\nthe first way of scaling has the advantage that we can immediately see whether the means of the two groups on the latent variables differ or not; here we found no significant difference\nsince the latent variables are measured in the same way in both groups, we continue with our actual research question, which concerns the latent regression model; regressing the latent variables on each other resulted in a model that is equivalent to the model for strong factorial invariance\nsubsequently, we constrained the regression coefficient across the two groups to see whether the effect of family of origin (FO) has the same effect on marital adjustment (MA) in the two groups; again, by doing a chi-square difference test we could determine whether or not this constraint was tenable; since the test was not significant, we may assume that the two regression slopes are the same across the groups\nnote that in the end, we still have the same warning as for the first model about a Heywood case (negative variance) in the first group; we should present such results in a paper; we could constrain this variance to zero (and we could do the same thing in the other group, because it is not significantly different from zero there either), which would make this indicator and the latent variable it measures identical; put differently, this implies that we can measure marital adjustment with this one variable, and we do not need the other one for it"
  },
  {
    "objectID": "day1_exercises.html#step-1-configural-invariance",
    "href": "day1_exercises.html#step-1-configural-invariance",
    "title": "Exercises",
    "section": "",
    "text": "If we specify the model for configural invariance:\n\nhow many sample statistics are there?\nhow many free parameters are there?\nhow many df are there?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nSample statistics. We have 5 observed variables in each group. Hence, we have 5*6/2 = 15 unique elements in the covariance matrix S for each group, plus 5 observed means, so 20 sample statistics per group. That makes 40 sample statistics in total.\nFree parameter. In each group we estimate:\n\n3 factor loadings (there are 5 factor loadings, but 2 are used for scaling)\n2 factor variances\n1 factor covariance\n5 residual variances\n5 intercepts\n\nThat makes 16 parameters per group; hence, 32 free parameters in total.\nDegrees of freedom. The degrees of freedom are therefore: \\(df = 40 - 32 = 8\\).\n\n\n\n\n\n\nSpecify the two-factor model for configural invariance.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nMODEL:      MA BY S I;\n            FO BY F M FM; \n\nMODEL g2:   MA BY S@1 I;\n            FO BY F@1 M FM;\n            [S I F M FM];\n            [MA@0 FO@0];\nwhere the first MODEL: command is the general model specification, and the second part MODEL g2; is overruling the multiple group factor analysis defaults through:\n\nfreeing the factor loadings (so they are not identical to the factor loadings in g1), and using @1 to ensure the factors are scaled\nfreeing the intercepts in the second group (so they are not identical to the intercepts in g1)\nfixing the latent means to zero (to ensure identification)\n\nAlternatively, one could also say:\nMODEL:      MA BY S I;\n            FO BY F M FM; \n\nMODEL g2:   MA BY I;\n            FO BY M FM;\n            [S I F M FM];\n            [MA@0 FO@0];\nHere, we are doing the same thing, but not overruling the default for the factor loadings of the first indicator of each latent variable (hence the initial scaling remains in tact).\nIn addition to this MODEL command, we specify the OUTPUT command as:\nOUTPUT: TECH1 MOD(4);\n\n\n\n\n\n\nRun this model, and check whether the TECH1 output matches your answer under Question 4 regarding where the free parameters are. Also check the warning(s); is there a reason for concern?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe get this warning:\nTHE MODEL ESTIMATION TERMINATED NORMALLY\n\nWARNING:  THE RESIDUAL COVARIANCE MATRIX (THETA) IN GROUP G1 IS NOT\nPOSITIVE DEFINITE.  THIS COULD INDICATE A NEGATIVE VARIANCE/RESIDUAL\nVARIANCE FOR AN OBSERVED VARIABLE, A CORRELATION GREATER OR EQUAL TO ONE\nBETWEEN TWO OBSERVED VARIABLES, OR A LINEAR DEPENDENCY AMONG MORE THAN TWO\nOBSERVED VARIABLES.  CHECK THE RESULTS SECTION FOR MORE INFORMATION.\nPROBLEM INVOLVING VARIABLE I.\nThis implies that the residual covariance matrix contains a combination of numbers (i.e., parameter estimates) that are not possible in a covariance matrix. We can start with checking the relevant parameter estimates in the first group:\nResidual Variances\n  S                528.470    130.512      4.049      0.000\n  I                -39.883    109.198     -0.365      0.715\n  F                 21.613     10.983      1.968      0.049\n  M                 53.509     11.710      4.570      0.000\n  FM               103.074     16.168      6.375      0.000\nIt shows that the residual variance for the indicator I (intimacy) is negative; this is not possible (as variances are by definition zero or larger). This is referred to as a Heywood case, and is typically interpreted as meaning that the model is too complicated for the data (for instance because the model is really wrong, or because the sample size is too small). I is an indicator of MA, which only has one other indicator (i.e., S); this is often a difficult situation in terms of estimation.\nOne action we could take is setting this residual variance to zero (note the parameter is not significantly different from zero). That would mean this variable is a perfect indicator of the latent variable MA, as there would be no measurement error at all; this would in turn mean we only need this indicator, and there is no need for the second indicator to measure the latent variable.\nBut for now, we will just leave the variable in, and consider the model fit before moving on to testing weak factorial invariance.\n\n\n\n\n\n\nFor now, we ignore this warning. What is the model fit?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nChi-Square Test of Model Fit\n\n          Value                              6.025\n          Degrees of Freedom                     8    \n          P-Value                           0.6444\nThe chi-square test indicates the model fits well (note btw that the degrees of freedom are indeed 8). We see this chi-square is simply the sum of the chi-squares that are obtained for each group, as presented in the output:\nChi-Square Contribution From Each Group\n\n          G1                                 4.688\n          G2                                 1.337\nThe other fit measures also indicate the model fits very well:\nRMSEA (Root Mean Square Error Of Approximation)\n\n          Estimate                           0.000\n          90 Percent C.I.                    0.000  0.095\n          Probability RMSEA &lt;= .05           0.786\n\nCFI/TLI\n\n          CFI                                1.000\n          TLI                                1.000\n\nSRMR (Standardized Root Mean Square Residual)\n\n          Value                              0.019"
  },
  {
    "objectID": "day1_exercises.html#step-2-weak-factorial-invariance",
    "href": "day1_exercises.html#step-2-weak-factorial-invariance",
    "title": "Exercises",
    "section": "",
    "text": "Next, specify the model for weak factorial invariance. How many df would this model have? Run this model and report on the model fit.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIn the model for weak factorial invariance, we set the factor loadings invariant across groups, while not constraining the mean structure. In practice, this implies we no longer have to overrule the defaults for the factor loadings, but we still need to overrule the defaults for the intercepts and latent means. Hence, the model specification becomes:\nMODEL:      MA BY S I;\n            FO BY F M FM; \n\nMODEL g2:   [S I F M FM];\n            [MA@0 FO@0];\nWhile in the previous model we were estimating 3 factor loadings in each group (so 6 factor loadings in total), we now estimate 3 factor loadings for both groups. Hence, this model has 3 free parameters less, and thus 3 df more than the model for configural invariance; the df should thus be 8+3=11. This also becomes clear when looking at the TECH1 output of this model (see the numbers that identify the free parameters in the matrix LAMBDA).\nWe see this in the chi-square test:\nChi-Square Test of Model Fit\n\n          Value                              8.735\n          Degrees of Freedom                    11\n          P-Value                           0.6463\nWe see indeed the number of df is 11.\n\n\n\n\n\n\nDo a chi-square difference test to determine whether the assumption of weak factorial invariance (i.e., equal factor loadings across groups) holds.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\(\\Delta \\chi^2 = 8.735 - 6.025 = 2.710\\)\n\\(\\Delta df = 11 - 8 = 3\\)\nHence, the chi-square difference is 2.710 with df=3. The p-value for this is 0.4385 (for an online chi-square calculator, see for instance: https://www.fourmilab.ch/rpkp/experiments/analysis/chiCalc.html).\nThis means that our H0 (i.e., weak factorial invariance) is not rejected. Put differently, the constraints for weak factorial invariance can be imposed."
  },
  {
    "objectID": "day1_exercises.html#step-3-strong-factorial-invariance",
    "href": "day1_exercises.html#step-3-strong-factorial-invariance",
    "title": "Exercises",
    "section": "",
    "text": "Next, specify the model for strong factorial invariance. Again, indicate how many df this model will have, run the model, and report on the model fit.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nStrong factorial invariance is the default multiple group factor model that Mplus runs. Hence, it does not require us to overrule any of its defaults. Compared to the previous model, this means that it will constrain all five intercepts to be identical across the two groups, but at the same time, it will freely estimate the two latent means in the second group. Hence, the difference in df is 5-2=3; the new model with have 3 df more than the one we had before.\nThe MODEL command for this model is very simple; we only need:\nMODEL:      MA BY S I;\n            FO BY F M FM; \nThe chi-square of this model is:\nChi-Square Test of Model Fit\n\n        Value                             15.647\n        Degrees of Freedom                    14\n        P-Value                           0.3354\nWe see that indeed the df are 3 more than before (\\(11 + 3 = 14\\)).\n\n\n\n\n\n\nDo a chi-square difference test to determine whether the assumption of strong factorial invariance (i.e., equal intercepts; only latent mean differences) holds.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe compare the current model fit to that of the previous model (i.e., the one for weak factorial invariance), to determine whether the additional constraints are tenable.\nHence we have:\n\\(\\Delta \\chi^2 = 15.647 - 8.735 = 6.912\\)\n\\(\\Delta df = 14 - 11 = 3\\)\nA chi-square difference test of 6.912 with 3 df has a p-value of 0.0748. Hence, again, the H0 is not rejected, which now means we can assume that strong factorial invariance holds.\n\n\n\n\n\n\nCheck the TECH1 output for the latter model. How do you see the constraints that were imposed?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYou can see the intercepts and factor loadings are constrained across the two groups, because the same numbers are used to show which ones are estimated freely:\n         NU\n            S             I             F             M             FM\n           ________      ________      ________      ________      ________\n                1             2             3             4             5\n\n\n         LAMBDA\n           MA            FO\n            ________      ________\n S                  0             0\n I                  6             0\n F                  0             0\n M                  0             7\n FM                 0             8"
  },
  {
    "objectID": "day1_exercises.html#alternative-way-of-scaling",
    "href": "day1_exercises.html#alternative-way-of-scaling",
    "title": "Exercises",
    "section": "",
    "text": "Above, we have used the usual way of scaling:\n\nfirst factor loading of each latent variable fixed to 1 (in each group)\nall latent means fixed to 0 (in each group), combined with estimating the intercepts for the observed variables freely (in each group)\n\nIn this case, the step from configural to weak factorial invariance now consists of only adding more constraints, which makes it easy to see these two models are nested (i.e., the model for weak factorial invariance is nested under the model for configural invariance).\nHowever, it is less obvious that the model for strong factorial invariance is nested under the model for weak factorial invariance, because it includes constraining parameters (i.e., the intercepts have to be equal across groups), but also requires freeing parameters (i.e., the latent means in the second group are freed).\nTo ensure that these models are nested, we use an alternative way of specifying these models, based on a different way of scaling. The key issue here is that these alternative model specification are simply reparameterizations of the the same models; they are equivalent models, as we can see because they lead to the exact same model fit.\n\n\nInstead of scaling with the latent means fixed to zero and the intercepts all estimated freely, scale the latent variable in the model for configural invariance through setting the intercept for the first indicator to zero, while allowing the latent mean to be estimated freely. Compare the model fit to the one obtained above for the configural invariance model.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe MODEL command can be specified as:\nMODEL:        MA BY S I;\n              FO BY F M FM;\n\nMODEL g1:     [S@0 F@0];\n              [MA FO];\n\nMODEL g2:     MA BY S@1 I;\n              FO BY F@1 M FM;\n              [S@0 I F@0 M FM];\nWhen running the model like this, it does not converge within the default number of iterations. We get this message:\nNO CONVERGENCE.  NUMBER OF ITERATIONS EXCEEDED.\nand there is no model fit, standard errors, or p-values. This can mean the model is too complicated. It can also mean you need to specify (better) starting values. As a first step, we can just increase the number of iterations, by adding:\nANALYSIS:   ITERATIONS ARE 20000;\nIn this case, this solves it, that is, the model now converges. When considering the model fit, we see it is exactly the same as that of the model discussed in Question 7.\nChi-Square Test of Model Fit\n\n          Value                              6.025\n          Degrees of Freedom                     8    \n          P-Value                           0.6444\nBecause the chi-square is exactly the same (and the df), this implies these models are statistically equivalent; they are identical, just parameterized differently. You cannot distinguish between them on statistical grounds.\nHence, this shows that this alternative way of scaling the latent variables gives a model that is equivalent.\n\n\n\n\n\n\nNext, use this alternative way of scaling and specify the model for weak factorial invariance.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe MODEL command can be specified as:\nMODEL:        MA BY S I;\n              FO BY F M FM;\n\nMODEL g1:     [S@0 F@0];\n              [MA FO];\n\nMODEL g2:     [S@0 I F@0 M FM];\nThat is, we no longer overrule the defaults for the factor loadings. Note that for this model to converge, we do not need to increase the number of iterations.\nWhen considering the model fit, we see it is exactly the same as that of the model discussed in Question 8:\nChi-Square Test of Model Fit\n\n          Value                              8.735\n          Degrees of Freedom                    11\n          P-Value                           0.6463\nThis means that the current model is statistically equivalent to the model we had before to impose weak factorial invariance.\n\n\n\n\n\n\nAnd now, the Moment Suprême: We will specify the model for strong factorial invariance using this alternative way of scaling. The key issue to notice here is that in going from weak factorial invariance to strong factorial invariance, we will now only add constraints, which makes it very clear that the latter model must be nested under the former.\nRemember that before, we were adding constraints (i.e., fixing the intercepts to be identical across the groups), but also were freeing parameters (i.e., allowing the latent means in the second group to be estimated freely); this makes it difficult to see that the model for strong factorial invariance is nested under that of weak factorial invariance.\nBut with this alternative way of scaling, which leads to statistically equivalant models (i.e., models with the exact fit, and which are thus indistinguishable), we now only add constraints on parameters when going from weak to strong factorial invariance, such that it is very obvious that (and how) they are nested.\nSpecify the model for strong factorial invariance using this alternative way of scaling.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe MODEL command can be specified as:\nMODEL:        MA BY S I;\n              FO BY F M FM;\n\nMODEL g1:     [S@0 F@0];\n              [MA FO];\n\nMODEL g2:     [S@0 F@0];\nThis model leads to the exactly same model fit as that of the model discussed in Question 10:\nChi-Square Test of Model Fit\n\n          Value                             15.647\n          Degrees of Freedom                    14\n          P-Value                           0.3354\nThis means that the current model is statistically equivalent to the model we had before to impose strong factorial invariance. This also means that we can now be certain that the model for strong factorial invariance is nested under the model for weak factorial invariance; this means we can do a chi-square difference test to compare them (as we already did above).\n\n\n\n\n\n\nWhile this alternative way of specifying the series of models is useful to seeing their nestedness, the first way of specifying the models also has advantages. One of these advantages is that it requires fewer defaults to be overruled. But more importantly, it allows us to easily determine in the strong factorial invariance model whether there are latent mean differences between the groups. Report and interpret these.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIn the first approach, the model for strong factorial invariance is based on constraining the factor loadings and intercepts across the groups, and freeing the latent means in the second group, while the means in the first group are fixed to zero.\nThat is, for G1 we get:\nMeans\n    MA                 0.000      0.000    999.000    999.000\n    FO                 0.000      0.000    999.000    999.000\nand for G2 we get:\nMeans\n    MA                -0.415      3.118     -0.133      0.894\n    FO                -3.140      1.655     -1.898      0.058\nThis implies that the significance test of the latter can be used to determine whether there are latent mean differences between the second and the first group. Here we get a p-value of 0.894 for MA, and a p-value of 0.058 for FO; hence for both we do not find evidence that they differ from zero, which means that for both we do not find evidence that the two groups differ."
  },
  {
    "objectID": "day1_exercises.html#full-model",
    "href": "day1_exercises.html#full-model",
    "title": "Exercises",
    "section": "",
    "text": "As both the test for weak and for strong factorial invariance were non-significant, we can conclude that strong factorial invariance holds. This means we can compare individuals from these groups, and we can compare the means of these groups. It also means that the constructs “Marital Adjustment” and “Family of Origin” are measured in the same way in these two groups, and that we can specify a structural model for these latent variables in the two groups to investigate how they are related.\n\n\nSpecify the model as initially intended, with FO as a predictor of MA at the latent level, while assuming strong factorial invariance.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe model is now specified as:\nMODEL:      MA BY S I;\n            FO BY F M FM; \n            MA ON FO;\n\n\n\n\n\n\nRun this model, and compare the model fit to that of the model for strong factorial invariance that we had before. What do you conclude?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThese models have the exact same model fit (same chi-square and df). This means these models are statistically equivalent. Hence, it does not matter whether you estimate the covariance between the two latent variables, or a regression coefficient.\n\n\n\n\n\n\nWhen comparing the TECH1 output of these two models, what difference do you spot?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIn the two-factor model we have for G1:\n          BETA\n              MA            FO\n              ________      ________\n MA                 0             0\n FO                 0             0\n\n\n           PSI\n              MA            FO\n              ________      ________\n MA                14\n FO                15            16\nthat is, no structural parameters, and a covariance is Psi; in contrast in the regression model we have for G1:\n               BETA\n                  MA            FO\n                  ________      ________\n     MA                 0            14\n     FO                 0             0\n\n\n               PSI\n                  MA            FO\n                  ________      ________\n     MA                15\n     FO                 0            16\nshowing there is a structural parameter in Beta (going to MA, coming from FO), and there is no covariance estimated in Psi. The same is true for G2.\n\n\n\n\n\n\nAs a final step, we want to investigate whether the regression coefficient is different in the two groups or not. How can you investigate this?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIn the model above, the regression parameter was not constrained across the groups. We can now specify a model in which we constrain the parameter to be invariant across the two groups; that model will be nested under the previous model, and we can thus do a chi-square difference test to see whether the constraint holds.\nTo constrain the parameter across the groups, we only have to give it a label (i.e., a name). Here we will give it the label c (you could als give it a longer name, such as coeff). The model is now specified as:\n        MODEL:      MA BY S I;\n                    FO BY F M FM; \n                    MA ON FO (c);\nTo see that this leads to having the same regression coefficient in both groups, we can check the TECH1 output. For G1 we have (only showing part of it):\n           ALPHA\n              MA            FO\n              ________      ________\n                    0             0\n\n\n           BETA\n               MA            FO\n              ________      ________\n MA                 0            14\n FO                 0             0\n\n\n          PSI\n             MA            FO\n              ________      ________\n MA                15\n FO                 0            16\nand for G2:\n         ALPHA\n             MA            FO\n             ________      ________\n                  22            23\n\n\n          BETA\n              MA            FO\n              ________      ________\n MA                 0            14\n FO                 0             0\n\n\n           PSI\n              MA            FO\n              ________      ________\n MA                24\n FO                 0            25\nIt shows that the structural parameter in both cases is the same parameter (number 14).\nThe model fit is:\nChi-Square Test of Model Fit\n\n          Value                             16.315\n          Degrees of Freedom                    15\n          P-Value                           0.3614\nshowing it has 1 df more than the model we had before (because we estimate 1 parameter less). Doing a chi-square difference test we get:\n\\(\\Delta \\chi^2 = 16.315 - 15.647 = 0.688\\) with \\(df-1\\), which gives a p-value of 0.4137, which means the test is not significant. Hence, H0 does not have to be rejected, and we can therefore assume that the regression coefficient is identical across the two groups."
  },
  {
    "objectID": "day1_exercises.html#conclusion",
    "href": "day1_exercises.html#conclusion",
    "title": "Exercises",
    "section": "",
    "text": "To summarize what we did in this exercise:\n\nwe were interested in investigating a full SEM model (with both a measurement model and a structural model), in two groups\nbefore we could investigate the structural part of the model (i.e., the latent regression), we had to determine whether there was measurement invariance across the groups\nto this end we ran a series of three models: configural invariance, weak factorial invariance and strong factorial invariance; these models are increasingly more restrictive, and we compared each subsequent model with the preceding one (using a chi-square different test) to see whether the additional constraints were tenable\nsince neither of the two chi-square difference test reached significance, we can conclude that strong factorial invariance holds; this means we are measuring the same constructs in both groups, and we can compare their latent means\nwe also considered an alternative way to scale the models (based on setting one of the intercepts per factor to zero, rather than the latent means), which more clearly shows that the model for strong factorial invariance is a special case of the model for weak factorial invariance\nthe first way of scaling has the advantage that we can immediately see whether the means of the two groups on the latent variables differ or not; here we found no significant difference\nsince the latent variables are measured in the same way in both groups, we continue with our actual research question, which concerns the latent regression model; regressing the latent variables on each other resulted in a model that is equivalent to the model for strong factorial invariance\nsubsequently, we constrained the regression coefficient across the two groups to see whether the effect of family of origin (FO) has the same effect on marital adjustment (MA) in the two groups; again, by doing a chi-square difference test we could determine whether or not this constraint was tenable; since the test was not significant, we may assume that the two regression slopes are the same across the groups\nnote that in the end, we still have the same warning as for the first model about a Heywood case (negative variance) in the first group; we should present such results in a paper; we could constrain this variance to zero (and we could do the same thing in the other group, because it is not significantly different from zero there either), which would make this indicator and the latent variable it measures identical; put differently, this implies that we can measure marital adjustment with this one variable, and we do not need the other one for it"
  },
  {
    "objectID": "day1_introduction.html",
    "href": "day1_introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "These exercises are for day 1 of S23. In the first exercise we investigate measurement invariance, while in the second exercise the focus is on path analysis. You can choose which of these you want to focus on, and start with that.\nIn both exercises we will focus on: a) determining the number of degrees of freedom (df) by counting the number of sample statistics (i.e., unique pieces of information in our data) and the number of free parameters; and b) checking the TECH1 output to see where the free parameters are in the diverse underlying model matrices.\nYou can navigate through these excises by using the left-hand menu. Answers are included below each exercise in a drop down option. The data and Mplus input files can found in the SURFdrive folder."
  },
  {
    "objectID": "day2_exercises.html",
    "href": "day2_exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "The file PTSD.dat contains data on burn survivors, specifically (in this order):\n\ngender (gender),\npercentage total body surface burned (tvlo),\nSVL at wave 1 (2 weeks after burn injury; W1),\nSVL at wave 2 (4 weeks after burn injury; W2),\nSVL at wave 3 (2 months after burn injury; W3),\nSVL at wave 4 (4 months after burn injury; W4),\nSVL at wave 5 (6 months after burn injury; W5),\nSVL at wave 6 (9 months after burn injury; W6),\nSVL at wave 7 (12 months after burn injury; W7),\nSVL at wave 8 (18 months after burn injury; W8), and\nBPAS at wave 9 (24 months after burn injury; pain).\n\n\n\nSpecify a latent growth curve model. Consider different specifications discussed in the lecture, and try to find the best specification. Use only the time measurements, not including additional predictor variables. Think about which metric of time to use and the shape of the function (linear or quadratic). Base your decision of the best model on plots (!), model fit indices, model comparison tools, and interpretation of the model parameters.\nIf you have reason to believe that another type of LGCM fits the data better, feel free to specific and estimate that model.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nDeciding on the metric of time Based on these descriptions, I’ve chosen for the following specification of time in the LGM:\ni s | W1@0.5 W2@1 W3@2 W4@4 W5@6 W6@9 W7@12 W8@18;\nIn this specification I set the first time point to 0.5 months after burn injury (approximation of 2 weeks after burn injury), the second time point to 1 month after burn injury (approximation of 4 weeks after burn injury), etc.\nPlots You can enable the plot functionality of Mplus by specifying\nPLOT:\n  TYPE = PLOT3;\n  SERIES = W1 W2 W3 W4 W5 W6 W7 W8 (s);\nThe TYPE = PLOT3; function ensures that all kinds of different types of plots are available in Mplus (see the Mplus User Guide for an overview of the different types of plots). The SERIES = ...; function tells Mplus to draw a line through the named variables in that order, for each individual.\nDeciding on linear vs. linear and quadratic slope Some example syntaxes for running models with different trajectory shapes are available in SURFdrive. These include:\n\nA LGCM with only a linear slope.\nA LGCM with an added quadratic slope.\nA LGCM with an added quadratic slope but no quadratic slope factor variance.\n\nFor the model with a quadratic slope, it was necessary to fix the variance of Q at 0 to ensure convergence. For both models, only CFI and TLI indicate adequate fit.\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nParameters\nAIC\nBIC\nRMSEA\nCFI\nTLI\nSRMR\n\n\n\n\nexercise1a.out\n13\n14051\n14096\n.14\n.93\n.94\n.08\n\n\nexercise1a_Q.out\n-\n-\n-\n-\n-\n-\n-\n\n\nexercise1a_Q0.out\n14\n14037\n14085\n.13\n.94\n.94\n.08\n\n\n\nModel comparison tools To see which model fit the data better, we can do a \\(\\Delta \\chi^{2}\\) (i.e., Chi-square difference) test (e.g., using the function chisq_sb() from the tidySEM package in R, or using on online \\(\\chi^{2} calculator\\)): \\(\\Delta \\chi^{2} = 16.14\\), \\(\\Delta df = 1\\), \\(p &lt; .001\\). Furthermore, both AIC and BIC are lower in the model with a quadratic slope. Thus, model misfit is significantly lower when the quadratic slope is added although the fit indices still do not indicate adequate fit. We can use the plots, RES option in Mplus, or modification indices to see what the source of the misfit is.\nModel parameters The mean of the quadratic slope factor is significant (q = 0.02, \\(p &lt; .001\\)), indicating that, on average, the growth curve does follow a quadratic curve (see exercise1a_Q0.out).\n\n\n\n\n\n\nUsing the best fitting LGM model found above, regress the growth parameters on TVLO and regress pain on the growth components. Then investigate if there are gender differences in the regression of the growth parameters on TVLO and in the regression of pain on the growth parameters?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nRephrasing the question, we are asked to investigate if gender moderates the predictive relationship of TVLO on the growth components, as well as the relationship between the growth components and Pain. As such, we can do a multigroup analysis by gender, resulting in the addition of the following syntax to the VARIABLE: command:\nGROUPING = gender (1 = male 2 = female);\nSince we needed to fix the quadratic slope variance to 0, we cannot estimate any regressions on the quadratic slope or use the quadratic slope as a predictor of some outcome. We therefore focus on the intercept and linear slope.\nThe file exercise1B.inp on SURFdrive contains the Mplus syntax on how to specify this model, including the MODEL TEST command to test between-group (i.e., between-gender) differences in the effect of TVLO on the growth components, and the growth components on pain. This is only an illustrative example for how to approach this analysis; your specific execution may differ (e.g., you could specify 2 models, and with across group constraints and one without, and then use the \\(\\Delta \\chi^{2}\\) test to see if the improvement of model fit is significant). Note that in this example, the Wald \\(\\chi^2\\) p-value is not significant. That means that there are no significant gender differences in the effect of the growth trajectory on pain.\nNote that the Wald test is an overall test of all comparisons that we specify in MODEL TEST. Thus, if you want a separate test for the regression of TVLO on the growth parameters, you need to re-run the analysis but with a different MODEL TEST argument.\nConclusion: There are no gender differences in the regression of growth parameters on TVLO and in the regression of Pain on the growth parameters."
  },
  {
    "objectID": "day2_exercises.html#burn-survivors",
    "href": "day2_exercises.html#burn-survivors",
    "title": "Exercises",
    "section": "",
    "text": "The file PTSD.dat contains data on burn survivors, specifically (in this order):\n\ngender (gender),\npercentage total body surface burned (tvlo),\nSVL at wave 1 (2 weeks after burn injury; W1),\nSVL at wave 2 (4 weeks after burn injury; W2),\nSVL at wave 3 (2 months after burn injury; W3),\nSVL at wave 4 (4 months after burn injury; W4),\nSVL at wave 5 (6 months after burn injury; W5),\nSVL at wave 6 (9 months after burn injury; W6),\nSVL at wave 7 (12 months after burn injury; W7),\nSVL at wave 8 (18 months after burn injury; W8), and\nBPAS at wave 9 (24 months after burn injury; pain).\n\n\n\nSpecify a latent growth curve model. Consider different specifications discussed in the lecture, and try to find the best specification. Use only the time measurements, not including additional predictor variables. Think about which metric of time to use and the shape of the function (linear or quadratic). Base your decision of the best model on plots (!), model fit indices, model comparison tools, and interpretation of the model parameters.\nIf you have reason to believe that another type of LGCM fits the data better, feel free to specific and estimate that model.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nDeciding on the metric of time Based on these descriptions, I’ve chosen for the following specification of time in the LGM:\ni s | W1@0.5 W2@1 W3@2 W4@4 W5@6 W6@9 W7@12 W8@18;\nIn this specification I set the first time point to 0.5 months after burn injury (approximation of 2 weeks after burn injury), the second time point to 1 month after burn injury (approximation of 4 weeks after burn injury), etc.\nPlots You can enable the plot functionality of Mplus by specifying\nPLOT:\n  TYPE = PLOT3;\n  SERIES = W1 W2 W3 W4 W5 W6 W7 W8 (s);\nThe TYPE = PLOT3; function ensures that all kinds of different types of plots are available in Mplus (see the Mplus User Guide for an overview of the different types of plots). The SERIES = ...; function tells Mplus to draw a line through the named variables in that order, for each individual.\nDeciding on linear vs. linear and quadratic slope Some example syntaxes for running models with different trajectory shapes are available in SURFdrive. These include:\n\nA LGCM with only a linear slope.\nA LGCM with an added quadratic slope.\nA LGCM with an added quadratic slope but no quadratic slope factor variance.\n\nFor the model with a quadratic slope, it was necessary to fix the variance of Q at 0 to ensure convergence. For both models, only CFI and TLI indicate adequate fit.\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nParameters\nAIC\nBIC\nRMSEA\nCFI\nTLI\nSRMR\n\n\n\n\nexercise1a.out\n13\n14051\n14096\n.14\n.93\n.94\n.08\n\n\nexercise1a_Q.out\n-\n-\n-\n-\n-\n-\n-\n\n\nexercise1a_Q0.out\n14\n14037\n14085\n.13\n.94\n.94\n.08\n\n\n\nModel comparison tools To see which model fit the data better, we can do a \\(\\Delta \\chi^{2}\\) (i.e., Chi-square difference) test (e.g., using the function chisq_sb() from the tidySEM package in R, or using on online \\(\\chi^{2} calculator\\)): \\(\\Delta \\chi^{2} = 16.14\\), \\(\\Delta df = 1\\), \\(p &lt; .001\\). Furthermore, both AIC and BIC are lower in the model with a quadratic slope. Thus, model misfit is significantly lower when the quadratic slope is added although the fit indices still do not indicate adequate fit. We can use the plots, RES option in Mplus, or modification indices to see what the source of the misfit is.\nModel parameters The mean of the quadratic slope factor is significant (q = 0.02, \\(p &lt; .001\\)), indicating that, on average, the growth curve does follow a quadratic curve (see exercise1a_Q0.out).\n\n\n\n\n\n\nUsing the best fitting LGM model found above, regress the growth parameters on TVLO and regress pain on the growth components. Then investigate if there are gender differences in the regression of the growth parameters on TVLO and in the regression of pain on the growth parameters?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nRephrasing the question, we are asked to investigate if gender moderates the predictive relationship of TVLO on the growth components, as well as the relationship between the growth components and Pain. As such, we can do a multigroup analysis by gender, resulting in the addition of the following syntax to the VARIABLE: command:\nGROUPING = gender (1 = male 2 = female);\nSince we needed to fix the quadratic slope variance to 0, we cannot estimate any regressions on the quadratic slope or use the quadratic slope as a predictor of some outcome. We therefore focus on the intercept and linear slope.\nThe file exercise1B.inp on SURFdrive contains the Mplus syntax on how to specify this model, including the MODEL TEST command to test between-group (i.e., between-gender) differences in the effect of TVLO on the growth components, and the growth components on pain. This is only an illustrative example for how to approach this analysis; your specific execution may differ (e.g., you could specify 2 models, and with across group constraints and one without, and then use the \\(\\Delta \\chi^{2}\\) test to see if the improvement of model fit is significant). Note that in this example, the Wald \\(\\chi^2\\) p-value is not significant. That means that there are no significant gender differences in the effect of the growth trajectory on pain.\nNote that the Wald test is an overall test of all comparisons that we specify in MODEL TEST. Thus, if you want a separate test for the regression of TVLO on the growth parameters, you need to re-run the analysis but with a different MODEL TEST argument.\nConclusion: There are no gender differences in the regression of growth parameters on TVLO and in the regression of Pain on the growth parameters."
  },
  {
    "objectID": "day2_exercises.html#alcohol-use",
    "href": "day2_exercises.html#alcohol-use",
    "title": "Exercises",
    "section": "Alcohol use",
    "text": "Alcohol use\nThe figure below depicts the basic LGCM for the alcohol use data from @duncan_introduction_2006, example 8_1.\n\n\n\nLatent growth curve model for alcohol use.\n\n\nThe data are in the file DDS8_1.dat, with variables ALC1YR1 ALC1YR2 ALC1YR3 ALCPROB5 AGE1 and GENDER1 (in that order). Missing values are coded as -99. The variable ALCPROB5 is categorical, it indicates alcohol problems in year 5 of the study (0 = no, 1 = yes).\n\nExercise 2A\nSet up the LGCM as depicted in the figure above in Mplus using the | notation (rather than specifying a CFA by using the BY statement). Inspect the output carefully with special attention for a) the pattern of missing values, b) the model fit, and c) the interpretation of the parameter estimates. How well does the model predict alcohol use of the years?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe input and output file (exercise2A.inp and exercise2A.out) can be found on SURFdrive. The table under PROPORTION OF DATA PRESENT in the output file shows that the majority of the cases is complete, but that there is a small amount of attrition (panel dropout). You can also inspect the coverage matrix to inspect how much information you have for different parts of the model. Regarding model fit, the model fits well overall with the chi-square test of model fit \\(\\chi^{2}(1) = 2.781\\), \\(p = 0.095\\), and the CFI and TLI above their recommended cutoff points. However, the RMSEA implies some some degree of misfit at \\(0.062\\). The intercept and slope means indicate a relatively high starting point (3.68, \\(SE = .081\\)) and a growth of 0.92 (\\(SE = .053\\)) per year. The intercept and slope factors show considerable variance, indicating that the starting points and rates of growth differ considerably across individuals. Interestingly, the explained variance \\(R^{2}\\) is high for years 1 and 3, but lower for year 2.\n\n\n\n\n\nExercise 2B\nWe will now explore how different predictor variables affect the model fit. Include gender and age in the model as predictors of the intercept and slope. Interpret the fit of the model and the output. Feel free to estimate several models, including or excluding certain covariates. Make a model fit table by hand in a spreadsheet, reporting on the fit indices you deem to be appropriate. Which model do you consider to be best?\n\n\n\n\n\n\nConfirmatory versus exploratory research\n\n\n\nWhen you conduct confirmatory research, and are testing theoretical hypotheses, you should not add and omit paths based on exploratory analyses and model fit. It is fine to add and remove paths in exploratory research. Model fit indices, like AIC and BIC, are suitable for selecting well-fitting models in exploratory research. p-values are not designed for variable selection, and using them for that purpose may lead to sub-optimal models.\nIt is good scientific practice to clearly separate confirmatory and exploratory research. When you conduct exploratory research, you should not perform inference on the resulting parameters based on p-values (because inference generalizes your findings to the population, and exploratory findings tend to be tailored toward this specific sample). You should also not present exploratory results as if they were testing a post-hoc theory (also referred to as “Hypothesizing After the Results are Known”, or HARKing). This is a questionable research practice and can lead to false-positive (spurious) findings.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe files exercise2B_M1.inp, exercise2B_M2.inp, and exercise2B_M3.inp, on SURFdrive contain various models in which we predict the growth components using AGE1 and GENDER1. The model fit remains excellent. After removing non-significant paths from exercise2B_M1.inp (order based on magnitude of standardized effect size), gender and age only predict the starting point but not the slope. However, after having removed the effect of gender and age on the slope, the model suddenly fits very badly (exercise2B_M2.inp). Careful inspection of the output shows that the covariance between intercept and slope has disappeared from the model (it is now the covariance between a latent variable and a residual, Mplus automatically puts these at zero). Mplus automatically constrains these to zero. If we add the statement I WITH S to the model, we obtain a good fit with significant effects of both gender and age on the intercept (exercise2B_M3.inp). This illustrates the importance of checking the output carefully to find out if Mplus is actually doing what you think it does!\n\n\n\n\n\nExercise 2C\nInclude alcohol problems in year 5 in the model. Let the intercept and slope factors predict alcohol problems in year 5. Declare the variable as categorical in the variable section (CATEGORICAL = ALCPROB5). Inspect if the effect of age and gender on alcohol problems year 5 is completely mediated by the growth factors, or if there are additional direct paths from age and gender on the alcohol problems.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe model fit is still good. Note that after adding a categorical dependent variable to the model, Mplus switches to a robust estimator (MLR, and the exact type of regression the Mplus uses now is a logit regression). Both intercept and slope predict alcohol problems (see exercise2c_M1.inp on SURFdrive). Age also predicts alcohol problems directly (see exercise2c_M2.inp on SURFdrive). Since age predicts alcohol problems both directly and via the intercept, a mediation analysis is in order. This shows that the indirect effect of age via the intercept on alcohol problems is still significant when the direct effect is added to the model."
  },
  {
    "objectID": "day2_exercises.html#level-and-shape-parameterization",
    "href": "day2_exercises.html#level-and-shape-parameterization",
    "title": "Exercises",
    "section": "Level and shape parameterization",
    "text": "Level and shape parameterization\nThe file GPA.dat holds the following variables (in that order):\n\ngrade point average (GPA) data with GPA scores of 200 students in 6 consecutive semesters (gpa1, …, gpa6),\nhigh school GPA highpa,\ngender sex, and\nadmitted to university of choice (missing if not applied for university, student).\n\nIn this exercise you will use the GPA data to set up a level and shape model (i.e., a LGCM with estimated time scores).\n\nExercise 3A\nUse a parameterization with GPA1@0 and GPA6@1. The loadings for the other time points should be freely estimated. This can be done with, for example, the syntax GPA2* as shown in the handout. Interpret the factor loadings and estimate for S.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe Mplus model syntax for this LGCM can be found in exercise3A.inp on SURFdrive. The factor loadings indicate the proportion of change for a 1 unit change in time (here, a 1 unit change of time is specified as the change between the first and the last time points). The predicted change in the outcome for a 1 unit change in time is the mean of the slope, \\(\\alpha_{S} = 0.55\\). Therefore, when looking at the factor loadings, 24% of the total change occurs between GPA1 and GPA2.. The intercept at GPA1 = 2.575. So the estimated score at GPA2 is \\(GPA1 = 2.575 + 0.239*0.549 = 2.706\\). The estimated score at GPA3 is \\(GPA3 = 2.575 + 0.450*0.549 = 2.822\\), etc.\n\n\n\n\n\nExercise 3B\nNow use a parameterization with GPA1@0 and GPA2@1. The other GPA’s should be freely estimated. Interpret the factor loadings and estimate for S.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe Mplus model syntax for this parametrization of the LGCM with freely estimated time scores can be found in exercise3B.inp. The mean of the slope factor \\(\\alpha_{S}\\) now indicates the difference between GPA1 and GPA2. The estimated factor loadings indicate the distance in units from the starting point, where 1 unit is S. In other words, every distance compares to the increase between GPA1 and GPA2.\n\n\n\nWhich parameterization do you like best?\n\n\nExercise 3C\nDraw the development of GPA over time, using the parametrization of your choice) based on your own calculations (by hand). Compare this to the estimated means plot that you can get with the plot command:\nPLOT: \n  SERIES = GPA1-GPA6 (s); \n  TYPE = PLOT3;\nDon’t forget that you need to rescale that plot, since S is linear while the location of the estimated points is based on the factor loadings.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe estimated means plot for the parametrization (0 1 * * * *) is shown below, and can be generated by clicking the plots button, and selecting “Estimated means”.\n\n\n\nEstimated means\n\n\n\n\n\n\n\nExercise 3D\nUse sex as a predictor of the intercept and slope and interpret the result (with 0 = boys, 1 = girls).\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe Mplus model syntax for the LGCM with time scores (0 * * * * 1) and gender predicting the growth components can be found in exercise3D.inp. Sex is a significant predictor of the intercept and a significant predictor of development, with girls having a higher initial level (\\(b = .079\\), \\(SE = .037\\)), and a greater development over time (\\(b = .136\\), \\(SE = .054\\)). Who run the world?"
  },
  {
    "objectID": "day2_exercises.html#latent-growth-curve-model-on-gpa-data",
    "href": "day2_exercises.html#latent-growth-curve-model-on-gpa-data",
    "title": "Exercises",
    "section": "Latent growth curve model on GPA data",
    "text": "Latent growth curve model on GPA data\n\nExercise 4A\nContinuing with the data used for the previous exercise, set up a latent growth model for GPA for the 6 consecutive occasions and run this model. Obtain the following parameters:\n\nAIC, BIC, \\(\\chi^{2}\\), RMSEA, CFI, and TLI;\nThe mean of the intercept factor \\(\\alpha_{I}\\) and slope factor \\(\\alpha_{S}\\);\nThe variance of the intercept facor \\(\\psi_{I}\\) and slope factor \\(\\psi_{S}\\).\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe Mplus model syntax for this can be found in exercise4A.inp.\n\n\n\n\n\nExercise 4B\nThen, set up a latent growth curve model for 3 years where each year is a latent variable measured by the GPA of two consecutive semesters.\nThe factor loadings for GPA2, GPA4 and GPA6 ought to be constrained to be equal with a label (a) behind the loading in the syntax. As such, the scores relate in the same way to the year score over time. The GPA intercepts are constrained at 0.\nIf you get the error message below, can you find out what the problem is?\n    WARNING:  THE LATENT VARIABLE COVARIANCE MATRIX (PSI) IS NOT POSITIVE DEFINITE. THIS \n    COULD INDICATE A NEGATIVE VARIANCE/RESIDUAL VARIANCE FOR A LATENT VARIABLE, A CORRELATION \n    GREATER OR EQUAL TO ONE BETWEEN TWO LATENT VARIABLES, OR A LINEAR DEPENDENCY AMONG MORE \n    THAN TWO LATENT VARIABLES. CHECK THE TECH4 OUTPUT FOR MORE INFORMATION. \nA rough way to deal with this problem may be to fix the problematic parameter to a particular value (e.g., .001), try this and rerun the model. Now examine the same parameters as for exercise 4a, and compare the two. Are there major differences?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe Mplus model syntax for this can be found in exercise4B_M2.inp. Note that, without year3@.001 (see exercise4B_M1.inp), this code gives an error message: The variance of the latent variable year3 is estimated negatively which is problematic since variances should always be positive. A simple way to deal with the problem of the latent variance of year3 is to fix it to a very small value (.001) for instance, as it would also be illogical to fix a variance to 0. To do this, simply add this to your input file under model: year3@.001;\nIf you inspect the output carefully (and provided you have requested standardized estimates) you will notice that the latent variables year2 and year3 have a correlation of 1. So the negative variance is the result of a multicolinearity problem. It is apparently better to analyze these data using only the observed variables gpa1-gpa6. Creating latent variables per year does not work well. In line with this interpretation, the fit and results of the simple latent growth model look better than the 2nd order latent growth curve model."
  },
  {
    "objectID": "day2_introduction.html",
    "href": "day2_introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "These exercises are for day 1 of S23. In this computer lab session you can practice with specifying latent growth models in Mplus and interpreting the output. All of the input files for the exercises are provided with the course materials on SURFdrive."
  },
  {
    "objectID": "day3_part1.html",
    "href": "day3_part1.html",
    "title": "Part 1: Latent Class Analysis and Latent Transition Analysis",
    "section": "",
    "text": "In this exercise we will explore the development of dating status over time using a latent transition analysis (LTA). However, we will slowly build up the model, starting with a latent class analysis (LCA) first at a single time points, before going to LTA. Use the data in DatingSex.dat, which holds data on five dating indicators measured at two occasions (u11, …, u15, u21, …, u25), as well as the variable gender. The \\(u\\)-variables represent five yes/no items measured at two time points (first digit represents time point, second digit represents the item)."
  },
  {
    "objectID": "day3_part1.html#exercise-1-latent-class-analysis",
    "href": "day3_part1.html#exercise-1-latent-class-analysis",
    "title": "Part 1: Latent Class Analysis and Latent Transition Analysis",
    "section": "Exercise 1: Latent Class Analysis",
    "text": "Exercise 1: Latent Class Analysis\nBefore we can perform an LTA, we first need to determine the number of classes at each time point using separate LCAs.\n\nExercise 1A: Number of classes\nDecide how many latent classes you think should be used. To determine this, first set up an LCA for the first time point (i.e., ignoring the variables at time point 2, as well as gender). Run an LCA for 2, 3, 4, and 5 classes, and collect information on the AIC, BIC, the LMR LRT (TECH11), the bootstrapped LRT (TECH14), and the size of the smallest class. Fill in the table below, and then decide how many classes you want to use for the next set of analyses. To help in your decision, you can also create the LCA item profiles using the SERIES function of the OUTPUT: command.\n\nInformation for selecting number of latent classes.\n\n\n\n\n\n\n\n\n\n\nNumber of classes\nAIC\nBIC\nLMR LRT\nBootstrapped LRT\nSize smallest class\n\n\n\n\n2\n\n\n\n\n\n\n\n3\n\n\n\n\n\n\n\n4\n\n\n\n\n\n\n\n5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe information for selecting the number of latent classes at the first time point has been filled in into the table below.\n\nInformation for selecting number of latent classes.\n\n\n\n\n\n\n\n\n\n\nNumber of classes\nAIC\nBIC\nLMR LRT\nBootstrapped LRT\nSize smallest class\n\n\n\n\n2\n6681\n6735\n\n\n321\n\n\n3\n6659\n6743\n\n\n216\n\n\n4\n6668\n6781\n\n\n160\n\n\n5\n6675\n6818\n\n\n136\n\n\n\nThe AIC indicates three classes, whereas the BIC is the lowest for the 2-classes LCA. In deciding between the two-classes or three-classes solution, I would let substantive arguments play the deciding role (i.e., see if the different classes can be given an substantive interpretation). For now, these exercises will continue with the two-classes solution.\n\n\n\n\n\nExercise 1B: Model convergence\nEvaluate if the two-classes LCA model converged to a global solution. In other words, do we have reason to believe that Mplus did not find the best solution for the parameter estimates (i.e., is there evidence for a local solution)?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nFor this, we need to check if our best loglikelihood value is replicated many times when the algorithm starts searching for solutions at different points in the parameter space. It would be problematic if we cannot replicate our best solution (i.e., the solution with the lowest loglikelihood) when we use different starting values: That would imply Mplus found local solutions, rather than ‘the best’ global solution. Setting the STARTS function in Mplus helps us check this. In the output, look for the RANDOM STARTS RESULTS RANKED FROM THE BEST TO THE WORST LOGLIKELIHOOD VALUES section. Here we see that the best loglikelihood value has been replicated approximately eight times out of ten (this number can change as the starting values are random), which is what we want.\n\n\n\n\n\nExercise 1C: Model fit\nContinuing with the two-classes solution. Have a look at the output, specifically the model fit information and the response pattern frequencies of TECH10. Evaluate the model fit. Can we use the Chi-Square test of model fit for this? Are there particular patterns in the data that the 2-classes LCA cannot replicate well?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nLooking at the Chi-Square Test of Model Fit, we can see that the Pearson Chi-Square and the Likelihood Ratio Chi-Square are very similar. This implies that we can actually used these tests to assess model fit. The p-value lower than 0.05 indicates the model does not fit well. Note, however, that the similarity of the Pearson Chi-Square and the Likelihood Ratio Chi-Square values is an exception rather than the rule. Especially when you have eight or more binary outcomes, it very likely that both values differ substantially, in which case you wouldn’t look at either of the tests.\nThe Standardized Residual (z-score) of the response patterns (in TECH10 of the output), can inform us about where the model misfit might come from. We are looking for absolute values greater than 1.96 to indicate significant differences between the observed frequencies and estimated frequencies. The most problematic response pattern is pattern 11, which is 00111, and also the most frequent pattern in the data. 94 individuals are observed to have such a pattern, but the model predicts only 67.85 individuals to have this. We can think of adjustments to the model to resolve this problem. For example, we might look at a three-classes solution after all, or we might inspect the BIVARIATE MODEL FIT INFORMATION to get further information about which two items are problematic for to model.\n\n\n\n\n\nExercise 1D: LCA model parameters\nNow look at the model output, specifically RESULTS IN PROBABILITY SCALE and LATENT CLASS INDICATOR ODDS RATIOS FOR THE LATENT CLASS. Which item (or items) is (are) the most predictive for which class you actually belong to?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe RESULTS IN PROBABILITY SCALE tells us for each class separately what the probability is of scoring a category 1 or category 2 for each item. These probabilities differ the most between the classes for items 3 and 5. This indicates that these two items are the most different across the classes, and therefore might also be the most useful for predicting which class you belong. This hunch is confirmed by results in the LATENT CLASS INDICATOR ODDS RATIOS FOR THE LATENT CLASS. Here we see that the category you belong to for items 3 and 5 have the highest odds ratio for going to latent class 1 (compared to latent class 2). In other words, scoring higher than category 1 on item U13 multiplies your odds of belonging to latent class 1 compared to latent class 2 by 8.067."
  },
  {
    "objectID": "day3_part1.html#exercise-2-latent-transition-analysis",
    "href": "day3_part1.html#exercise-2-latent-transition-analysis",
    "title": "Part 1: Latent Class Analysis and Latent Transition Analysis",
    "section": "Exercise 2: Latent Transition Analysis",
    "text": "Exercise 2: Latent Transition Analysis\nFor the LTA, we are going to extend the LCA of exercise 1 to two time points, and additionally include a structural relationship between the latent classes at time point 1 and time point 2.\n\nExercise 2A: Setting up the LTA\nSet up a model with two latent class variables for the two time points, say c1 and c2. Exclude the variable gender for now (i.e., explore the LTA without covariates first), and assume there are 2 latent classes. Restrict the thresholds (and hence response probabilities) across the two time points by first repeating the thresholds for each latent class, in both MODEL c1 and MODEL c2. To be sure Mplus does what you want, include equality constraints on the five thresholds of %c1#1% and %c2#1%, and similarly for %c1#2% and %c2#2%. Note that these constraints can be interpreted as imposing measurement invariance over time. Although we don’t test the assumption of measurement invariance in these exercises, it is definitely something you would want to check in practice.\nAfter running the analysis, inspect the proportions of yes/no answers for each of the indicators in the latent classes (look at probability scale in Mplus output).\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe parameters in probability scale can be found in under the heading RESULTS IN PROBABILITY SCALE in the output file. For example, we find that the probability of falling into category 1 of the item U11 (i.e., answering yes) is 0.434 (\\(SE = 0.024\\)) if you are in class 1 at the first time point, and 0.566 (\\(SE = 0.024\\)) for falling into category 2 (i.e., answering yes). Because of the imposed constraints, the probabilities also apply to time point 2, see Latent Class C2#1.\n\n\n\n\n\nExercise 2B: Estimated transition probabilities\nExamine the proportions of participants in each class, based on the estimated model. Note that for each latent variable, the total proportions add up to 1. Next, examine the latent transition probabilities based on the estimated model. What do these probabilities signify?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYou can find proportions of participants in each class under the heading FINAL CLASS COUNTS AND PROPORTIONS FOR EACH LATENT CLASS VARIABLE BASED ON ESTIMATED POSTERIOR PROBABILITIES. These probabilities represent the proportion of the total sample that is assigned to each class. Note that an individual can have a non-zero probability of being assigned to both classes. For example, there might be a 70% probability that the person belongs to class 1, and a 30% probability that the person belongs to class 2. The proportions here are a sum across those probabilities for all participants. Thus, this person would contribute for 30% to the proportion of the sample in class 2.\nYou can find the latent transition probabilities based on the estimated model under the heading TECHNICAL 15 OUTPUT. These probabilities represent the probability that an individual assigned to one class at time one, will be assigned to another class at time 2. So for example, we see that people in class 1 at time 1 also tend to be in class 1 at time 2 (.76 probability).\n\n\n\n\n\nExercise 2C: Covariates\nIf there is time, you can conduct additional analyses. Include gender as a control variable on the observed variables.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYou could include gender as a control variable on the observed variables, by adding it to the USEVARIABLES, and including the following lines:\nu11-u15 ON gender; u21-u25 ON gender;\nSee exercise1C.inp. Alternatively, you could regress class membership on gender, to see whether men are more likely to be in a particular class than women, or vice versa. This is only allowed when you’re NOT using probability parametrization. So, you would have to remove this line:\nPARAMETERIZATION = PROBABILITY;\nAnd add this line:\nc1 c2 ON gender;"
  },
  {
    "objectID": "day3_part2.html",
    "href": "day3_part2.html",
    "title": "Part 2: Latent growth (mixture) modeling",
    "section": "",
    "text": "The goal of this exercise is to find subpopulations with different alcohol use trajectories. To this end, we start with an exploratory latent class growth analysis, and work towards a growth mixture model."
  },
  {
    "objectID": "day3_part2.html#exercise-3-latent-class-growth-analysis",
    "href": "day3_part2.html#exercise-3-latent-class-growth-analysis",
    "title": "Part 2: Latent growth (mixture) modeling",
    "section": "Exercise 3: Latent Class Growth Analysis",
    "text": "Exercise 3: Latent Class Growth Analysis\nFirst, we perform an exploratory latent class growth analysis (LCGA) as initial exploratory option.\n\nExercise 3A: LCGA\nSet up LCGA models models for 1, 2, 3, and 4 classes for the data in DDS8_1.dat. Specify the model using the | notation, and constrain the variances of the intercept and slope factors to be equal to 0. Furthermore, request TECH11, and TECH14 to help evaluate model fit.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nMplus model syntax for the LCGA models for 1, 2, 3, and 4 classes are in exercise2A_1C.inp, exercise2A_2C.inp, exercise2A_3C.inp, exercise2A_4C.inp, respectively.\n\n\n\n\n\nExercise 3B: Model convergence\nThese models use random starting values. Several independent random starts are made, to ensure that the model converges on the proper solution. The default is 20 random sets of starting values, of which 4 are run to completion. Inspect the output, and look carefully if the model estimation has converged, especially for the larger number of classes. Look for warning and error messages, make sure you understand what they are telling you.\nThe STARTS option is used to specify the number of initial random starting values and final stage optimizations. Now, increase the number of starts to ensure proper convergence. Once you are confident that the model has converged to the proper solution, compare the different models using the available fit information (e.g., BIC, LMR-LRT, BLRT, entropy, min. N in classes, etc.). Which model do you prefer, and why?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYou can inspect the convergence of the model by checking that the best final state loglikelihood value has been replicated using different starting values. See the output section RANDOM STARTS RESULTS RANKED FROM THE BEST TO THE WORST LOGLIKELIHOOD VALUES. Increasing the number of random starts can be done by specifying STARTS = 50 10; in the ANALYSIS command, where the first number specifies the initial number of random starts, and the second number specifies the number of initial starts that will be converged to the final stage.\nBased on the fit indices of the fitted LCGA models, I would select a 3-class model. The fit indices and (LMR-LRT tests essentially indicate that you can keep adding classes and improve the model, which makes it difficult to decide. However, if we look at the counts in each class, we see that from 4 classes onward, the smallest class has less than 10% of cases assigned to it. The minimum posterior classification probability and entropy are best for the 3-class model, which means that this model can reasonably accurately assign individuals to classes."
  },
  {
    "objectID": "day3_part2.html#exercise-4-growth-mixture-model",
    "href": "day3_part2.html#exercise-4-growth-mixture-model",
    "title": "Part 2: Latent growth (mixture) modeling",
    "section": "Exercise 4: Growth Mixture Model",
    "text": "Exercise 4: Growth Mixture Model\nTo allow for individual difference in the growth components within each class, we are going to extend the LCGA model to an GMM.\n\nExercise 4A:\nSet up the same models as analyzed in the previous exercise, but now allow the means and variances of the intercept and slope factors to be freely estimated in each class (i.e., a growth mixture model). You do this by mentioning the intercept and slope explicitly in the class-specific part of the syntax. This is a more complex model, and we might therefore expect that we will need fewer classes for a good description of the data. This analysis will also take more computing time, so add PROCESSORS = 4 to the analysis section. Make a table of the fit indices, look at BIC, the LMR-LRT (TECH11), and the bootstrapped LRT value (TECH14).\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nMplus model syntax for the GMM models for 1, 2, 3, and 4 classes are in exercise4A_1C.inp, exercise4A_2C.inp, exercise4A_3C.inp, exercise4A_4C.inp, respectively. The BLRT for the GMM with 3 classes warns that OF THE 10 BOOTSTRAP DRAWS, 7 DRAWS HAD BOTH A SMALLER LRT VALUE THAN THE OBSERVED LRT VALUE AND NOT A REPLICATED BEST LOGLIKELIHOOD VALUE FOR THE 3-CLASS MODEL. You can increase the number of random starts for the BLRT using LRTSTARTS = 0 0 40 10; in the ANALYSIS command. Also note the convergence problems for the GMM with 4 classes. This indicates that a model with 4 classes is likely to be too complex for the data. Furthermore, since the GMM with only a single class is equivalent to a “simple” LGCM, we focus on the GMM with 2 and 3 classes in the next exercises.\n\n\n\n\n\nExercise 4B\nPlotting the model-predicted trajectories makes it easier to interpret the model. Moreover, visualizing the raw data provides yet another way to evaluate the fit of your mixture model to the data. With this in mind, plot the GMM models with 2 and 3 classes you created in exercise 1c, and interpret what you see. First, plot only the predicted trajectories. Then, plot raw data as well. Explain the benefit of plotting the raw data in your own words.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nPlotting the raw data helps us understand how representative the average trajectory for each class captures the individual trajectories of individuals in that class. It helps us see how separable the classes are visually, instead of just relying on statistics like entropy. To get plots, make sure you include:\nPLOT:\n    TYPE = PLOT3;\n    SERIES = ALC1YR1(1) ALC1YR2(2) ALC1YR3(3);\nwhen running the model. Then press the “plot” button and select “Estimated mean and observed individual values” and “Estimated means and estimated individual values”. Below you can find these plots for the GMM with 2 classes.\n\n\n\nEstimated means and individual values.\n\n\n\n\n\nEstimated means and estimated invidivual trajectories\n\n\nAdmittedly, this plot looks chaotic. Primarily because the individual values are not colored according to the class to which they are assigned. Alternatively, you could get the estimated means and (individual) values per class in a separate window as well. However, MplusAutomation has some useful plotting functions that I recommend you to explore, for example plotGrowthMixtures(), in which you can color the lines according to their class membership.\n\n\n\n\n\nExercise 4C\nCovariates are often added to mixture models, to (a) predict class membership, (b) explain variation in the growth parameters within the classes, or (c) as a distal outcome. Whenever covariates are however added to the model, they change the latent class solution. Sometimes, this is fine, as the covariates can help to improve the classification. In other cases, you would use a 3-step approach, which Mplus has automated:\n\nFit an unconditional LCA (without covariates).\nA “most likely class variable” is created using the posterior distribution of step 1.\nThis most likely class variable is then regressed on (a) covariate.\n\nThere are a few options for how to do 3-step analysis. They all rely on adding to the VARIABLE command. For more info, see https://www.statmodel.com/download/webnotes/webnote15.pdf.\n\n\nCommands for conducting a 3-step model\nYou can add the following options to the VARIABLE command:\n\nAUXILIARY = x(R)\nThis is actually a 1-step method for predicting latent class memberships using Pseudo-Class draws.\nAUXILIARY = x(R3step);\nA 3-step procedure, where covariates predict the latent class.\nAUXILIARY = y(e)\nA 1-step method, where the latent class predicts a continuous distal outcome.\nAUXILIARY = y(de3step);\nA 3-step procedure, where latent class predicts continuous covariates (distal outcome) with unequal means and equal variances.\nAUXILIARY = y(du3step);\nA 3-step procedure, where latent class predicts continuous covariates (distal outcome) with unequal means and variances.\nAUXILIARY = Y(dcon);\nProcedure for continuous distal outcomes as suggested by Lanza et al (2013).\nAUXILIARY = Y(dcon);\nProcedure for categorical distal outcomes as suggested by Lanza et al (2013).\nAUXILIARY = y(BCH);\nImproved and currently best 3-step procedure with continuous covariates as distal outcomes.\n\nPick your final model from the previous exercise, and add both age and gender as auxiliary variables in the model. Try to think what 3-step model you want, and if you are not sure, run different models, so you can evaluate how the different procedures make a difference. What is the effect of both age and gender?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nI’m providing an example using the 3-step procedure in exercise4C.inp. The results of adding these covariate to the model as predictors of the latent class can be found under the heading TESTS OF CATEGORICAL LATENT VARIABLE MULTINOMIAL LOGISTIC REGRESSIONS USING THE 3-STEP PROCEDURE. It can be seen, from the overall test and the pairwise comparisons, that the third group is significantly older, and has a significantly lower proportion of girls than the other two classes."
  },
  {
    "objectID": "day4_CLPMvRICLPM.html",
    "href": "day4_CLPMvRICLPM.html",
    "title": "Part 1: The CLPM and RI-CLPM",
    "section": "",
    "text": "In the first part of the computer lab, we focus predominantly on Phase III of the causal research process, specifically the fitting of the traditional cross-lagged panel model (CLPM) and the random intercept cross-lagged panel model (RI-CLPM) to data. This is to get you acquainted with popular longitudinal SEM models in the literature. We want to emphasize that for your own research, Phases I and II are equally important as the estimation of statistical models. See Mulder et al. (2024) and Mulder, Usami, and Hamaker (2024) for examples of how cross-lagged panel models fit within the (causal) potential outcomes framework.\nWe are going to analyze data that were reported in Davies et al. (2016). The summary data (means, standard deviations and correlation matrix) are included in Davies.dat, and contains the means, standard deviations, and the correlation matrix. The number of observations is 232. There are 5 waves of data, taken when the child was 7, 8, 13, 14, and 15 years old. The order of the variables is:\n\nChild gender\nParental education\nInterparental hostility (waves 1-5): composite score based on observational data and questionnaires, reflecting the degree of hostility between the parents\nInterparental dysphoria (waves 1-5): based on composite score based on observational data and questionnaires, reflecting the degree of dysphoria\nChild/adolescent insecurity in the relationship with the parents (waves 1-5)\nPsychological problems (waves 1-5): based on the subscales anxious/depressed, withdrawal, aggressive behaviors, and delinquency scales of the Child Behavior Checklist (CBCL), filled out by both parents.\n\nHere we will focus on Interparental dysphoria and Psychological problems of the child. The DATA and VARIABLE commands should be:\nDATA: \n  TYPE = MEANS STDEVIATIONS CORRELATION;\n  FILE = Davies.dat;\n  NOBSERVATIONS = 232;\n\nVARIABLE: \n  NAMES = ChildGen ParentEd\n  Hos1 Hos2 Hos3 Hos4 Hos5 Dys1 Dys2 Dys3 Dys4 Dys5\n  Ins1 Ins2 Ins3 Ins4 Ins5 PsPr1 PsPr2 PsPr3 PsPr4 PsPr5;\n  USEVARIABLES = Dys1-Dys5 PsPr1-PsPr5;\n\nExercise A\nHow many sample statistics are there for this data set (focusing on the 5 measures of dysphoria and the 5 measures of psychological problems)?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThere are 10 observed variables such that there are \\(\\frac{10*11}{2} = 55\\) unique elements in the observed covariance matrix S, and 10 observed means in M. Therefore, there are 65 sample statistics in total.\n\n\n\n\n\nExercise B\nWe begin with an RI-CLPM. For now, do not impose any constraints on the parameters across time. Draw the model, and indicate which parameters will be estimated freely. How many parameters will be estimated in total? So how many df are there?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIn the RI-CLPM we estimate:\n\n2 variances for the random intercepts,\n1 covariance between the random intercepts,\n2 variances for the within-person centered variables at wave 1,\n1 covariance between the within-person centered variables at wave 1,\n8 residual variances (for the dynamic errors of both variables at wave 2-5),\n4 covariances between the residuals (for the dynamics errors at waves 2-5),\n16 lagged parameters (4 for each interval), and\n10 means.\n\nIn total, we estimate 44 parameters such that we have \\(65 - 44 = 21\\) df.\n\n\n\n\n\nExercise C\nRun the model. Check whether the number of df is correct. Also look at the TECH1 output, to see if you understand where the free parameters are. What is the model fit?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe input for this model is in RICLPM.inp. The model means are estimated in the \\(\\nu\\)-matrix, no parameters are estimated in the \\(\\theta\\)-matrix (measurement error variances), \\(\\lambda\\)-matrix (factor loadings), or \\(\\alpha\\)-matrix (means/intercepts of the latent variables). The variances and covariance of the random intercepts, the within-person centered variables at wave 1, and the dynamic errors at subsequent waves are all estimated in the \\(\\psi\\)-matrix. The lagged regression coefficients are estimated in \\(\\beta\\).\nApart from the \\(\\chi^{2}\\)-test of model, all fit indices indicate at least acceptable fit.\n\n\\(\\chi^{2} (21) = 41.451\\), \\(p = .005\\),\nRMSEA = 0.065,\nCFI = 0.979,\nTLI = 0.956, and\nSRMR = 0.029.\n\n\n\n\n\n\nExercise D\nInclude the significant standardized parameter estimates for the covariances (i.e., the WITH statements) and the lagged regression parameters (i.e., the ON statements) in the figure below. Indicate which part of the model is considered the between-person part (i.e., only person-specific, not changing over time), and which part is the within-person part (i.e., both person- and time-specific).\n\n\n\nThe bivariate random-intercept cross-lagged panel model with 5 repeated measures (waves).\n\n\n\n\nExercise E\nOmit the random intercepts. How many parameters and df does this model have? What is the model fit?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe input for this model is CLPMasRICLPM.inp. The model has three parameters less (and thus 3 df more) than the previous model: 2 variances and the covariance for the random intercepts.\nThe model fit indices show that this model does not fit well:\n\n\\(\\chi^{2} (24) = 73.374\\), \\(p &lt; .001\\),\nRMSEA = 0.094,\nCFI = 0.950,\nTLI = 0.907, and\nSRMR = 0.061.\n\n\n\n\n\n\nExercise F\nSpecify the CLPM and run this model. Compare it to the previous two models. How are these models related?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe input for this model is in CLPM.inp. This model is statistically identical to the previous model; these are different parameterizations of the same model. The model fit is therefore also exactly the same. Hence, this model is a special case of the RI-CLPM.\nComparing the two models using a chi-square difference test gives: \\(\\Delta \\chi^{2} = 73.37 - 41.45 = 31.92\\) with \\(24 – 21 = 3\\) df, \\(p &lt; .001\\). Hence, the random intercepts should not be omitted; put differently, there are stable, trait-like difference between families in the two variables (parental dysphoria and psychological problems).\nHowever, when constraints are placed on the bound of the parameter space (which is the case here, fixing a variance to 0 is its absolute minimum value), we should actually use the chi-bar-square test (\\(\\bar{\\chi}^{2}\\)-test; Stoel et al. 2006). The traditional \\(\\Delta \\chi^{2}\\)-test does not take into account that variances can only be positive and is therefore conservative. This means that if it is significant, we are certain that the correct test (i.e., the \\(\\bar{\\chi}^{2}\\) test) would also be significant. On the other hand, when the usual chi‐square test is not significant, we do not know anything about the result of the correct test (it can be significant or not significant).\nIf you are working in R with the lavaan package, you can find more information about the \\(\\bar{\\chi}^{2}\\)-test at jeroendmulder.github.io/RI-CLPM/lavaan.html#(bar{chi}^{2})-test. For Mplus users, there is a Shiny app by Rebecca Kuiper available as well.\n\n\n\n\n\nExercise G\nInclude the significant standardized parameter estimates for the covariances and the lagged regression parameters in the figure below.\n\n\n\nThe bivariate cross-lagged panel model with 5 repeated measures (waves).\n\n\n\n\nExercise H\nDiscuss how the model results differ.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nCross-lagged relationships In the RI-CLPM none of the cross-lagged parameters are significant. In contrast, in the CLPM there is a positive relationship from PsPr1 to Dys2. This implies that higher levels of children’s psychological problems at age 7 are followed by higher levels of interparental dysphoria at age 8. Moreover, from age 14 to 15 both cross-lagged parameters are significant and positive, indicating that psychological problems are followed by increases in interparental dysphoria, but also that increased interparental dysphoria is followed by an increase in psychological problems for the adolescent.\nAutoregressive parameters The autoregressive parameters in the RI-CLPM are lower, and have larger SE’s, such that fewer reach significance. This is expected as within-person stability is now captures in the random intercepts, rather than in the autoregressive effects in the CLPM.\nCorrelations In the CLPM only the residual correlation at wave 2 is significant; it is negative, indicating that external effects tend to have an opposite effect on these two processes; increases in Dysphoria are accompanied by decreases in psychological problems and vice versa. In the RI-CLPM, the within-person correlation at wave 1 is not significantly different from zero; however, at waves 2, 3 and 4 the correlations between the residuals is significant and negative. At wave 5 the residual variance is not significant.\nIn the RI-CLPM there is also the correlation between the random intercepts (i.e., the trait-like difference between families). This turns out to be a very substantial correlation of .63: Hence, in contrast to the results from the CLPM and the within-level results from the RI-CLPM, there is a strong positive relationship between trait-like levels of interparental dysphoria and trait-like levels of psychological problems.\n\n\n\n\n\n\n\n\nReferences\n\nDavies, Patrick T., Meredith J. Martin, Jesse L. Coe, and E. Mark Cummings. 2016. “Transactional Cascades of Destructive Interparental Conflict, Children’s Emotional Insecurity, and Psychological Problems Across Childhood and Adolescence.” Development and Psychopathology 28 (3): 653–71. https://doi.org/10.1017/S0954579416000237.\n\n\nMulder, Jeroen D., Kim Luijken, Bas B. L. Penning de Vries, and Ellen L. Hamaker. 2024. “Causal Effects of Time-Varying Exposures: A Comparison of Structural Equation Modeling and Marginal Structural Models in Cross-Lagged Panel Research.” Structural Equation Modeling: A Multidisciplinary Journal. https://doi.org/10.1080/10705511.2024.2316586.\n\n\nMulder, Jeroen D., Satoshi Usami, and Ellen L. Hamaker. 2024. “Joint Effects in Cross-Lagged Panel Research Using Structural Nested Mean Models.” Structural Equation Modeling: A Multidisciplinary Journal. https://doi.org/https://doi.org/10.1080/10705511.2024.2355579."
  },
  {
    "objectID": "day4_monteCarlo.html",
    "href": "day4_monteCarlo.html",
    "title": "Part 2: Studying Cross-Lagged Panel Models using Monte Carlo Simulations",
    "section": "",
    "text": "In the second part of the computer lab, we will be studying statistical properties of various cross-lagged panel models using the Monte Carlo functionality of Mplus. This is a somewhat different approach than the previous days of this course, but nevertheless very insightful. A useful source of information for performing simulation studies using Mplus is the Mplus User Guide, Chapter 12. The answer input files can be found on SURFdrive."
  },
  {
    "objectID": "day4_monteCarlo.html#exercise-1-the-traditional-cross-lagged-panel-model",
    "href": "day4_monteCarlo.html#exercise-1-the-traditional-cross-lagged-panel-model",
    "title": "Part 2: Studying Cross-Lagged Panel Models using Monte Carlo Simulations",
    "section": "Exercise 1: The traditional Cross-Lagged Panel Model",
    "text": "Exercise 1: The traditional Cross-Lagged Panel Model\nFirst, lets get ourselves acquainted with performing simulations in Mplus. Suppose want to study the power for detecting a nonzero cross-lagged effect in the traditional cross-lagged panel model. Such questions concerning power can also be investigated using simulations. In these cases, the population model (the model from which the data is generated) is the same as the estimation model. We then generate data with a nonzero cross-lagged effect, and estimate the CLPM on the generated data. We hope, of course, that the fitted CLPM returns a significant cross-lagged effect; if not, this would be a Type II error (i.e., not finding a significant effect, whereas we know there is one).\n\nExercise 1A: What to expect?\nBefore running the simulation study, let us think about what results we expect. We are using the same model for data generation and estimation. Do we expect bias in the parameter estimates? If we are testing significance with a significance level of \\(\\alpha = 0.05\\), what should be the 95% coverage rate for the parameters?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWhen the population model and the estimation model are the same, we do not expect a bias in the parameter estimates, nor in the standard errors of the parameter estimates. In Mplus, you get the average (average across all replications) estimate for each parameter. This average should be very close to the population values (i.e., the Truth). In this situation, the 95% coverage rate should be close to 95%. This means that in 95% of the replications, the confidence interval around the parameter estimates contains the population value. This is actually the definition of a confidence interval.\n\n\n\n\n\nExercise 1B: Simulation results for cross-lagged effect X to Y\nOpen the file MonteCarlo_template.inp, which is a template for Monte Carlo studies in Mplus. Complete the template to perform a power analysis for the traditional CLPM, for a sample size of 500, and a cross-lagged effect of X to Y of 0.2. For the MONTECARLO: command, name the variables to be created x1-x4 and y1-y4. Set the number of replications to a thousand, and set a seed.\nFor the MODEL POPULATION: command, define a traditional CLPM. Set the autoregressive effects of X to 0.3, and thoses of Y to 0.5. Set the cross-lagged effects of Y to X to 0.05, and those from X to Y to 0.2 (i.e., the parameter that we want to compute the power for). Set the variances of X and Y at the first wave to 1, and its covariance to 0.3. Set the residual variances of X to 0.9015, and those of Y to 0.670; set the residual covariances to 0.083. The mean structure can be set to 0 for all variables.\nFor the MODEL: command, define the same model. However, don’t use the @ symbol to fix parameter to particular values (as done in MODEL POPULATION:), but use the asterisk * to indicate what the parameter values should be. This way the parameters are estimates, but in the output file, Mplus knows what value (i.e., what Truth) the parameter values should be compared to.\nRun the simulations, and assess the bias, 95% coverage rate, and power for the cross-lagged effect of X to Y.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBias, coverage rate, and power can be read off in the MODEL RESULTS part of the output. For bias, we need to compare the average estimate to the population value. Because simulation studies contain a random component, the results are likely to be slighlty different for everyone (unless you use the same seed). However, the average estimate of each parameter approximately the same as their population values. The 95% coverage rates should all be approximately 95%. Power is defined as the percentage of times that the nonzero parameter is actually estimated to be significantly different from 0. For a cross-lagged effect of 0.2 and a sample size of n = 500, the power should be approximately 1.\n\n\n\n\n\nExercise 1C: Simulation results for cross-lagged effect Y to X\nNow look at the smaller cross-lagged effect, that of Y to X. What is the power for an effect of 0.05, given this sample size? Suppose that we have some missing data, how will this affect the power of this parameter?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe cross-lagged effect of Y to X is set to be a smaller effect size. Therefore, we can also see that the power is much smaller, approximately around 0.20 to 0.23. If we have missing data, this implies your sample size is smaller, and the power will therefore decrease."
  },
  {
    "objectID": "day4_monteCarlo.html#exercise-2-the-random-intercept-cross-lagged-panel-model",
    "href": "day4_monteCarlo.html#exercise-2-the-random-intercept-cross-lagged-panel-model",
    "title": "Part 2: Studying Cross-Lagged Panel Models using Monte Carlo Simulations",
    "section": "Exercise 2: The Random Intercept Cross-Lagged Panel Model",
    "text": "Exercise 2: The Random Intercept Cross-Lagged Panel Model\nNow, let us investigate what can go wrong if the data actually have some stable, between-person differences in them, but we do not account for this when analyzing the data. More specifically, what happens when we generate data with the RI-CLPM, but fit the traditional CLPM to the data? This is a case that was investigated at length by Hamaker, Kuiper, and Grasman (2015).\n\nExercise 2A: Simulation RI-CLPM vs. CLPM\nAdjust the Mplus input file that you created for Exercise 1. This time, in the MODEL POPULATION: command, define an RI-CLPM. Set the variances of the random intercept factors for X and Y to 1, and their covariance to 0.4. Create within-components, and set the autoregressions of the within-components for X to 0.15, and the autoregressions of the within-components of Y to 0.25. For the cross-lagged effects between the within-components, let those from X to Y be 0.2 again, and the other way around 0.05. Set the variances of the within-components of X and Y at the first wave to 1, and its covariance to 0.3. Set the residual variances of the within-components of X to 0.972, and those of Y to 0.8775; set the residual covariances to 0.148. The mean structure can be set to 0 for all variables.\nFor the MODEL: command, define the traditional CLPM. However, don’t use the @ symbol to fix parameter to particular values (as done in MODEL POPULATION:), but use the asterisk * to indicate what the parameter values should be.\nRun the simulations, and assess the bias of (a) the autoregressive effects, and (b) the cross-lagged effects.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThis situation is a case of model misspecification (which can also happen of course in reality). We see that the autoregressive effects are estimated to be much higher on average (the average is approximately 0.50-0.60 for X and approximately 0.55-0.65 for Y) than they should be. In other words, the autoregressive effects in the CLPM are biased when the data are actually generated by the RI-CLPM. The reason why the CLPM autoregressive effects are much higher is that these parameters now need to account not only for moment-to-moment stability, but also stability across the study span (i.e., the stability due to the random intercept factor). When analyzing empirical data, this is a phenomenon we commonly see: The autoregressive effects are estimated to be much smaller when analyzing data with the RI-CLPM, compared to analysis using the traditional CLPM.\nSimilarly, there is bias for the cross-lagged effects, but there is not a clear pattern here. For the cross-lagged effects of Xto Y, the bias is towards to 0, whereas for the cross-lagged effect of Y to X the bias is away from 0.\nThese finding are in line with Hamaker, Kuiper, and Grasman (2015), who concludes that using the traditional CLPM when the data are actually generated by the RI-CLPM, you may:\n\nfind spurious effects;\nfail to detect relationships;\nidentify the wrong variable as being causally dominant; or\nobtain the wrong sign for an effect.\n\n\n\n\n\n\nExercise 2B: Simulation RI-CLPM vs. CLPM\nNow, you might wonder what happens the other way around: What if we analyze data without stable, between-person differences, with the RI-CLPM? We will study this case now. Specify the population model from Exercise 1B, but analyse the generated data using the RI-CLPM. Assess the bias of the lagged effects.\n\n\n\n\n\n\nNote\n\n\n\nAdjust the “starting value” of model under the MODEL: command accordingly. Otherwise, in the Mplus output, the parameter estimates are compared to the wrong population parameter values. For example, you would expect that the variances of the random intercept factors are 0 when the data were generated under the traditional CLPM.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThere appears to be no bias in the lagged effects of the RI-CLPM, even when the data were generated by the traditional CLPM. This is to be expected as the traditional CLPM is nested under the RI-CLPM."
  },
  {
    "objectID": "day4_monteCarlo.html#exercise-3-the-stable-trait-autoregressive-trait-state-model",
    "href": "day4_monteCarlo.html#exercise-3-the-stable-trait-autoregressive-trait-state-model",
    "title": "Part 2: Studying Cross-Lagged Panel Models using Monte Carlo Simulations",
    "section": "Exercise 3: The Stable Trait Autoregressive Trait State model",
    "text": "Exercise 3: The Stable Trait Autoregressive Trait State model\nMeasurement error in measurements in psychology (and related disciplines) is the rule, rather than the exception. Therefore, we would ideally like to control for measurement error (as well as stable, between-person differences). This can be done using the Stable Trait Autoregressive Trait State (STARTS) model.\n\nExercise 3A: Fitting the STARTS model I\nAdjust the Mplus input file of Exercise 2A. This time, in MODEL POPULATION:, rather than fixing the measurement error variances to 0, fix the measurement error to 0.6. Given the same population parameter values of the other parameters in the model, this would imply a reliability of 0.7 of the single indicator (i.e., 30% measurement error variance, 70% true score variance). For the estimation in the MODEL: command, specify the STARTS model as well. Compared to the RI-CLPM, this implies simply freeing up the measurement error variances.\nCheck the MODEL RESULTS in the output. What do you notice?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nAs mentioned in the lecture, the STARTS model is notoriously hard to estimate. In fact, the model results will likely show only zeros. This is because in none of replications, the STARTS model actually converged. This can be seen in the SUMMARY OF ANALYSIS part in the output, see “Number of replications” and then “Completed”.\n\n\n\n\n\nExercise 3B: Ignoring measurement error\nOkay, that was annoying. But perhaps we can simply ignore measurement error? We can investigate if this has any effect on the estimates of interest, the lagged effects. Setup a simulation study to investigate potential bias in the estimates of the RI-CLPM, when the data actually contain measurement error. More specifically, generate data using the STARTS, but analyse the data using the RI-CLPM. What is the effect of unmodelled measurement error on the lagged effects of the RI-CLPM?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nTo study this research question, you can specify the STARTS model in the MODEL POPULATION: command, and specify the RI-CLPM under MODEL:. When looking at the model results, we can see that ignoring measurement error it not a good idea: The lagged effects are biased. However, there does appear to be a consistent pattern here: The lagged effects are estimated to be smaller than they should be (i.e., there is bias towards 0). This implies that in this particular situation, it is possible that we miss nonzero effects that are actually in the data (but that we estimate to be close to zero because of the unmodelled measurement error).\n\n\n\n\n\nExercise 3C: Fitting the STARTS model II\nLet’s see what we can do to actually make the STARTS model, from Exercise 3A, converge. Try out several options, and see if you can get simulation results. For example, try increasing the sample size, imposing constraints on the measurement error variances of time, using a Bayesian estimator, or a combination of these.\n\nImposing constraints. By imposing constraints on parameters, you make the estimation model simpler, and therefore Mplus can use the same amount of information to estimate fewer parameters. Think about what are sensible constraints to impose on the model. For example, would you constrain all measurement error variances to be the same in the model, or is it more sensible to constrain the measurement error variances of X and Y separately?\nIncrease sample size. Increasing the sample size might help, although you could ask if in practice increases in sample size are worth the effort? Perhaps it is more useful to collect additional waves of data, rather than collecting additional persons?\nBayesian. Bayesian estimation is a different statistical framework, that for estimation not only relies on the likelihood of the data, but also on prior information. Lüdtke, Robitzsch, and Ulitzsch (2023) and Lüdtke, Robitzsch, and Wagner (2018) have demonstrated how the STARTS model can be estimated in a Bayesian framework. Of specific interest here is the specification of the priors. For now, you can check out the simulation results by relying on the default prior in Mplus.\n\n\n\n\n\n\n\nWarning\n\n\n\nBayesian estimation relies on sampling from the posterior distribution, and can be incredibly slow, especially for difficult to fit models like the STARTS model. Try out what happens when you go Bayesian. You can always stop the Mplus operations by pressing Control-C in the MS-DOS window, or pressing the X.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe SURFdrive folder contains Mplus output files for each of these scenarios.\n\nImposing constraints. One option would be to constrain the measurement error variances over time for the X and Y variables separately. This might be more plausible than constraining all measurement error variances to be equal to each other, considering that X and Y are different measures. Imposing this constraint, as well as increasing the sample size to 1000, does aid convergence. The completed number of replications has now increased to 300-400, although this is still far from satisfactory. The convergence issues also in the model results, which still shows biased results. If all STARTS models had converged, then we should have no bias. Clearly, imposing constraints itself is not enough to help convergence in this particular situation.\nIncrease sample size. Increasing sample size even to 10000 does not help the situation: There are still no completed replications. An alternative might be to increase both the sample size, and the number of repeated measures.\nBayesian. We can invoke Bayesian estimation in the ANALYSIS: command by including ESTIMATOR = BAYES;. On its own, this option is not enough. Convergence is incredibly slow, let alone running 1000 replications. Therefore you might try both combining imposing the constraints, and adjusting the prior specification as recommended by Lüdtke, Robitzsch, and Ulitzsch (2023)."
  },
  {
    "objectID": "day4_monteCarlo.html#conclusion",
    "href": "day4_monteCarlo.html#conclusion",
    "title": "Part 2: Studying Cross-Lagged Panel Models using Monte Carlo Simulations",
    "section": "Conclusion",
    "text": "Conclusion\nUsing simulations in Mplus, we have studied some characteristics of popular cross-lagged panel models. Such simulations are useful to develop some intuition of how these models compare, and what happens when there is model misspecification (when the MODEL POPULATION: does not match the MODEL:). In reality, however, we do not know which model was the true population model. Based on theory and literature, we can of course reason about the presence of, for example, measurement error and unobserved heterogeneity, but there will always remain some residual uncertainty. This is one of the major challenges that applied researchers face.\nThese simulations have just been the tip of the iceberg. They can be extended to investigate all kinds of other scenario’s, for example, the impact of nonlinear relationships on estimates from linear cross-lagged panel models, the bias of parameter estimates in the presence of unmeasured confounding, etc. However, these scenario’s can become quite complex, and careful planning is needed to setup a simulation study,and to draw valid conclusions from it."
  },
  {
    "objectID": "day4_part1.html",
    "href": "day4_part1.html",
    "title": "Part 1: The quasi-simplex model",
    "section": "",
    "text": "In this first part of the computer lab session we will focus on the quasi-simplex model. We are going to study the concept of life satisfaction. The covariance matrix is in the file Coenders.dat and comes from 1724 children and adolescents that participated in the National Survey of Child and Adolescent Well-Being (NSCAW) in Russia. They indicated how satisfied they were with their lives as a whole on a 10-point scale (1 = not at all satisfied, 10 = very satisfied). There were three waves (1993, 1994 and 1995). At the third wave, the question was asked twice (with 40 minutes in between). Hence, in total there are four measurements obtained at three waves. The data and variables commands for these data should read:\nDATA: \n  TYPE = COVARIANCE;\n  FILE = Coenders.dat;\n  NOBSERVATIONS = 1724;\n\nVARIABLE: \n  NAMES = Y1 Y2 Y3 Y4;\nAll of the input files for the exercises are provided with the course materials on SURFdrive.\n\nExercise A\nThe researchers are interested in fitting a quasi-simplex model to these data, that is, a simplex model at the latent level, thus accounting for measurement error in the observations. This model is graphically represented in the slides. Provide the names of the variances (i.e., indicate in which model matrix, and which position in this matrix they have) in the quasi-simplex graph in the slides. What is the difference between the \\(e\\)’s and the \\(\\zeta\\)’s?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\(e\\)’s are residuals at the measurement level and can be found in the \\(\\theta\\)-matrix. They only influence the observation at a single occasion in time. \\(\\zeta\\)’s are residuals of the simplex process and can be found in the \\(\\psi\\)-matrix. Their effect is carried forward to future observation through the autoregressive relationships. This difference allows Mplus to estimate both types of ``errors’’.\n\n\n\n\n\nExercise B\nHow would you specify the model in Mplus?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nSee the Mplus input file Exercise B.inp.\n\n\n\n\n\nExercise C\nDetermine the number of degrees of freedom for this model (indicate how you obtained this number). Is it possible to estimate this model?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThere are \\(\\frac{4 \\times 5}{2} = 10\\) unique elements in \\(S\\).\nFree parameters:\n\n4 residual variances at measurement level\n1 factor variance\n3 residual factor variances\n3 regression parameters\n11 parameters in total.\n\nTherefore, we have \\(10 - 11 = -1\\) df. It is not possible to estimate this model because we are trying to estimate more parameters than we have information in the data.\n\n\n\n\n\nExercise D\nTo make sure a quasi-simplex model is identified, often the variances of the measurement errors are constrained to be equal over time. How can you do this in Mplus? How many df does this model have?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nSee the Mplus input file Exercise E.inp for how to constrain the measurement error variances over time. With constrained measurement error variances, we estimate 3 parameters less. So, we estimate \\(11 - 3 = 8\\) and therefore have \\(10 - 8 = 2\\) df.\n\n\n\n\n\nExercise E\nRun the model and report on the model fit.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWe find the below fit indices:\n\n\\(\\chi^{2}\\) = 13.29 with df = 2, \\(p = .0013\\),\nRMSEA = .057,\nCFI = .994, and\nTLI = .981.\n\nExcept for \\(\\chi^{2}\\) test of model fit, the model seems to fit the data well. Note, however, that the sample size is very large and therefore the \\(\\chi^{2}\\) is likely to be significant, even for minor problems with model fit.\n\n\n\n\n\nExercise F\nThe quasi-simplex model you just ran, led to the following warning:\nWARNING: THE LATENT VARIABLE COVARIANCE MATRIX (PSI) IS NOT POSITIVE DEFINITE. \nTHIS COULD INDICATE A NEGATIVE VARIANCE/RESIDUAL VARIANCE FOR A LATENT VARIABLE, \nA CORRELATION GREATER OR EQUAL TO ONE BETWEEN TWO LATENT VARIABLES, OR A LINEAR \nDEPENDENCY AMONG MORE THAN TWO LATENT VARIABLES. CHECK THE TECH4 OUTPUT FOR MORE \nINFORMATION. PROBLEM INVOLVING VARIABLE ETA4.\nWhat is the problem?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIn your output, look at the reported (estimated) residual variances. We find that the residual variance of ETA4 is estimated to be negative. This is a Heywood case and it is causing the warning to appear. Note however, that it is significant, so ``just” fixing it to 0 as a solution is probably not warranted here.\n\n\n\n\n\nExercise G\nAs indicated in the description of the data, the third and fourth measurement were obtained at the same measurement wave (with only 40 minutes in between). Hence, the researchers proposed the following model instead of the regular quasi-simplex model. Explain why this model makes more sense for these data than the regular quasi-simplex model. Tip: check the description of the study at the beginning of this exercise.\n\n\n\nAdjusted quasi-simplex model.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nAt each occasion there is a latent variable which represents Life Satisfaction. At the first two occasions there was only a single indicator of this latent variable, but at the third occasion there were two indicators.\n\n\n\n\n\nExercise H\nHow many df does this model have? Note that we keep the constraint on the variances of the measurement errors.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThere are \\(\\frac{4*5}{2} = 10\\) unique elements in S. We freely estimate:\n\n1 constrained residual variances at measurement level\n1 factor variance\n2 residual factor variances\n2 regression parameters\n1 factor loading\n7 parameters in total.\n\nTherefore, we have \\(10 - 7 = 3\\) df.\n\n\n\n\n\nExercise I\nAre these two models nested? If so, how? If not, why not, and how could we compare them?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYes, they are nested: this model is a special case of the previous model, as it is based on having ETA3 and ETA4 from the previous model now being a single latent variable. That is, we can constrain the residual variance of ETA4 to zero to get the alternative model. This gives us 1 df for the difference.\n\n\n\n\n\nExercise J\nSpecify this model in Mplus and run it. Report on the model fit.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nSee the Mplus input file Excercise J.inp for the model specification in Mplus. Apart from the \\(\\chi^{2}\\)-test of model fit, the model fits well:\n\n\\(\\chi^{2} (3) = 27.37\\), with \\(p &lt; .001\\),\nRMSEA = 0.069,\nCFI = 0.987,\nTLI = 0.973, and\nSRMR = 0.039.\n\n\n\n\n\n\nExercise K\nCompare the two models to each other. What can you conclude?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nComparing both models using the \\(\\Delta \\chi^{2}\\)-test gives us \\(27.37 – 13.29 = 14.0\\) with 1 df such that \\(p &lt; .001\\). This implies that imposing the restriction is not tenable. You can calculate the p-value of the \\(\\Delta \\chi^{2}\\) using the pchisq()-function in R (with the lower.tail argument set to FALSE), or an online tool\nComparing the models using information criteria gives us AIC = 29593 and BIC = 29637 for the first model, and AIC = 29605 and BIC = 29643 for the second. In conclusion, all measures indicate the first model is better. However, the current model makes more theoretical sense, and the negative variance estimate in the first model is a problem. For these 2 reasons, we should prefer the current model.\n\n\n\n\n\nExercise L\nCan you improve the second model in any way? Indicate which parameter you would add to your model, and what this parameter represents in substantive terms.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYou can get the modification indices by adding MOD to the OUTPUT command. Here, the suggested BY statements make no sense (later life satisfaction as an indicator of previous life satisfaction). With regards to the ON statement, only the suggested effect of ETA3 ON ETA1 makes sense as we then predict forwards in time. The WITH statement suggests adding a covariance between … (see the next exercise).\n\n\n\n\n\nExercise M\nRun a model in which you include the Y3 WITH Y4 parameter. Where will this relationship end up in the model? Does it lead to a significant improvement? How would you interpret this additional parameter?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nSee Exercise M.inp for the Mplus specification of this model. The Y3 WITH Y4 parameter is an additional covariance between the residuals of y3 and y4 (so not between y3 and y4 themselves). If we add this covariance and look at the standardized results, we get a correlation. This correlation actually quite high: \\(.522\\) (SE = .044), \\(p &lt; .001\\).\nModel fit is quite good (except for the \\(\\chi^{2}\\)-test of model fit):\n\n\\(\\chi^{2} (2) = 7.077\\), \\(p = .0291\\),\nRMSEA = 0.038,\nCFI = 0.997,\nTLI = 0.992, and\nSRMR = 0.011.\n\nTo compare this model to the previous model, we can do a the \\(\\Delta \\chi^{2}\\)-test: \\(27.37 – 7.08 = 20.29\\), with 1 df such that \\(p &lt; .001\\), which implies that adding the covariance between the residuals leads to a significant improvement in model fit. This additional parameter implies that y3 and y4 have more in common with each other than what would be expected based on their common dependence on ETA3."
  },
  {
    "objectID": "day4_part2.html",
    "href": "day4_part2.html",
    "title": "Part 2: The CLPM and RI-CLPM",
    "section": "",
    "text": "For the cross-lagged panel model (CLPM) and the random intercept cross-lagged panel model (RI-CLPM) we are going to analyze data that were reported in Davies et al. (2016). The summary data (means, standard deviations and correlation matrix) are included in Davies.dat, and contains the means, standard deviations, and the correlation matrix. The number of observations is 232. There are 5 waves of data, taken when the child was 7, 8, 13, 14, and 15 years old. The order of the variables is:\n\nChild gender\nParental education\nInterparental hostility (waves 1-5): composite score based on observational data and questionnaires, reflecting the degree of hostility between the parents\nInterparental dysphoria (waves 1-5): based on composite score based on observational data and questionnaires, reflecting the degree of dysphoria\nChild/adolescent insecurity in the relationship with the parents (waves 1-5)\nPsychological problems (waves 1-5): based on the subscales anxious/depressed, withdrawal, aggressive behaviors, and delinquency scales of the Child Behavior Checklist (CBCL), filled out by both parents.\n\nHere we will focus on Interparental dysphoria and Psychological problems of the child. The DATA and VARIABLE commands should be:\nDATA: \n  TYPE = MEANS STDEVIATIONS CORRELATION;\n  FILE = Davies.dat;\n  NOBSERVATIONS = 232;\n\nVARIABLE: \n  NAMES = ChildGen ParentEd\n  Hos1 Hos2 Hos3 Hos4 Hos5 Dys1 Dys2 Dys3 Dys4 Dys5\n  Ins1 Ins2 Ins3 Ins4 Ins5 PsPr1 PsPr2 PsPr3 PsPr4 PsPr5;\n  USEVARIABLES = Dys1-Dys5 PsPr1-PsPr5;\n\nExercise A\nHow many sample statistics are there for this data set (focusing on the 5 measures of dysphoria and the 5 measures of psychological problems?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThere are 10 observed variables such that there are \\(\\frac{10*11}{2} = 55\\) unique elements in the observed covariance matrix S, and 10 observed means in M. Therefore, there are 65 sample statistics in total.\n\n\n\n\n\nExercise B\nWe begin with an RI-CLPM (see slide 53). For now, do not impose any constraints on the parameters across time. Draw the model, and indicate which parameters will be estimated freely. How many parameters will be estimated in total? So how many df are there?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIn the RI-CLPM we estimate:\n\n2 variances for the random intercepts,\n1 covariance between the random intercepts,\n2 variances for the within-person centered variables at wave 1,\n1 covariance between the within-person centered variables at wave 1,\n8 residual variances (for the dynamic errors of both variables at wave 2-5),\n4 covariances between the residuals (for the dynamics errors at waves 2-5),\n16 lagged parameters (4 for each interval), and\n10 means.\n\nIn total, we estimate 44 parameters such that we have \\(65 - 44 = 21\\) df.\n\n\n\n\n\nExercise C\nRun the model. Check whether the number of df is correct. Also look at the TECH1 output, to see if you understand where the free parameters are. What is the model fit?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe input for this model is in RICLPM.inp. The model means are estimated in the \\(\\nu\\)-matrix, no parameters are estimated in the \\(\\theta\\)-matrix (measurement error variances), \\(\\lambda\\)-matrix (factor loadings), or \\(\\alpha\\)-matrix (means/intercepts of the latent variables). The variances and covariance of the random intercepts, the within-person centered variables at wave 1, and the dynamic errors at subsequent waves are all estimated in the \\(\\psi\\)-matrix. The lagged regression coefficients are estimated in \\(\\beta\\).\nApart from the \\(\\chi^{2}\\)-test of model, all fit indices indicate at least acceptable fit.\n\n\\(\\chi^{2} (21) = 41.451\\), \\(p = .005\\),\nRMSEA = 0.065,\nCFI = 0.979,\nTLI = 0.956, and\nSRMR = 0.029.\n\n\n\n\n\n\nExercise D\nInclude the significant standardized parameter estimates for the covariances (i.e., the WITH statements) and the lagged regression parameters (i.e., the ON statements) in the figure below. Indicate which part of the model is considered the between-person part, and which part is the within-person part.\n\n\n\nThe bivariate random-intercept cross-lagged panel model with 5 repeated measures (waves).\n\n\n\n\nExercise E\nOmit the random intercepts. How many parameters and df does this model have? What is the model fit?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe input for this model is CLPMasRICLPM.inp. The model has three parameters less (and thus 3 df more) than the previous model: 2 variances and the covariance for the random intercepts.\nThe model fit indices show that this model does not fit well:\n\n\\(\\chi^{2} (24) = 73.374\\), \\(p &lt; .001\\),\nRMSEA = 0.094,\nCFI = 0.950,\nTLI = 0.907, and\nSRMR = 0.061.\n\n\n\n\n\n\nExercise F\nSpecify the CLPM and run this model. Compare it to the previous two models. How are these models related?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe input for this model is in CLPM.inp. This model is statistically identical to the previous model; these are different parameterizations of the same model. The model fit is therefore also exactly the same. Hence, this model is a special case of the RI-CLPM.\nComparing the two models using a chi-square difference test gives: \\(\\Delta \\chi^{2} = 73.37 - 41.45 = 31.92\\) with \\(24 – 21 = 3\\) df, \\(p &lt; .001\\). Hence, the random intercepts should not be omitted; put differently, there are stable, trait-like difference between families in the two variables (parental dysphoria and psychological problems).\nHowever, when constraints are placed on the bound of the parameter space (which is the case here, fixing a variance to 0 is its absolute minimum value), we should actually use the chi-bar-square test (\\(\\bar{\\chi}^{2}\\)-test; Stoel et al. 2006). The traditional \\(\\Delta \\chi^{2}\\)-test does not take into account that variances can only be positive and is therefore conservative. This means that if it is significant, we are certain that the correct test (i.e., the \\(\\bar{\\chi}^{2}\\) test) would also be significant. On the other hand, when the usual chi‐square test is not significant, we do not know anything about the result of the correct test (it can be significant or not significant).\nIf you are working in R with the lavaan-package, you can find more information about the \\(\\bar{\\chi}^{2}\\)-test at jeroendmulder.github.io/RI-CLPM/lavaan.html#(bar{chi}^{2})-test. For Mplus users, there is a Shiny app by Rebecca Kuiper available as well.\n\n\n\n\n\nExercise G\nInclude the significant standardized parameter estimates for the covariances and the lagged regression parameters in the figure below.\n\n\n\nThe bivariate cross-lagged panel model with 5 repeated measures (waves).\n\n\n\n\nExercise H\nDiscuss how the model results differ.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nCross-lagged relationships In the RI-CLPM none of the cross-lagged parameters are significant. In contrast, in the CLPM there is a positive relationship from PsPr1 to Dys2. This implies that higher levels of children’s psychological problems at age 7 are followed by higher levels of interparental dysphoria at age 8. Moreover, from age 14 to 15 both cross-lagged parameters are significant and positive, indicating that psychological problems are followed by increases in interparental dysphoria, but also that increased interparental dysphoria is followed by an increase in psychological problems for the adolescent.\nAutoregressive parameters The autoregressive parameters in the RI-CLPM are lower, and have larger SE’s, such that fewer reach significance. This is expected as within-person stability is now captures in the random intercepts, rather than in the autoregressive effects in the CLPM.\nCorrelations In the CLPM only the residual correlation at wave 2 is significant; it is negative, indicating that external effects tend to have an opposite effect on these two processes; increases in Dysphoria are accompanied by decreases in psychological problems and vice versa. In the RI-CLPM, the within-person correlation at wave 1 is not significantly different from zero; however, at waves 2, 3 and 4 the correlations between the residuals is significant and negative. At wave 5 the residual variance is not significant.\nIn the RI-CLPM there is also the correlation between the random intercepts (i.e., the trait-like difference between families). This turns out to be a very substantial correlation of .63: Hence, in contrast to the results from the CLPM and the within-level results from the RI-CLPM, there is a strong positive relationship between trait-like levels of interparental dysphoria and trait-like levels of psychological problems.\n\n\n\n\n\n\n\n\nReferences\n\nDavies, Patrick T., Meredith J. Martin, Jesse L. Coe, and E. Mark Cummings. 2016. “Transactional Cascades of Destructive Interparental Conflict, Children’s Emotional Insecurity, and Psychological Problems Across Childhood and Adolescence.” Development and Psychopathology 28 (3): 653–71. https://doi.org/10.1017/S0954579416000237."
  },
  {
    "objectID": "day5_exercises.html",
    "href": "day5_exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "In Model 1 we first decompose each variable at each time point (occasion) in two parts:\n\na person-specific mean, which has the same value at each occasion but differs from person to person\na deviation from that mean for each person and occasion.\n\nHence, we decompose the variables into two latent variables: One with person-specific means, at the between level. One with deviations from these means unique to each occasion and person, at the within level. These deviations can be seen as simply the within-person centered scores for a person at each occasion.\nNext, we add two regression relationships:\n\nBetween level: the person means of Somber are regressed on the person means of Event\nWithin level: the person-mean centered variable Somber is regressed on the person-mean centered variable Event.\n\nNote that this is not a dynamic SEM model yet in terms of the Mplus specification, because there are no explicit lagged relationships in the model. But note that there is an implicit lagged relationship: Although the two variables are measured at the same occasion (as indicated by the subscript t), the variable Event refers to the interval between t-1 and t, while the variable Somber is referring to the specific time point t. Hence, the reported on events take place before the somber feelings.\nThis model can be represented as depicted below, with on the left this decomposition, and on the right the regression models specified at each level.\n\n\n\nTo run this model, use the input file model1.inp. In the ANALYSIS command you will find a number of commands that set up the Bayesian multilevel estimation procedure:\nANALYSIS:   TYPE = TWOLEVEL;\n            ESTIMATOR = BAYES; \n            PROC = 2;\n            BITER= (2000);\n            BSEED = 9556;\nMake sure the following commands for OUTPUT and PLOT are included:\nOUTPUT:     TECH1 STDYX;\nPLOT:       TYPE = PLOT3;\n            FACTORS = ALL;\nCheck out the model specification (i.e., the MODEL command). What do the commands on the within person level and on the between person level do?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nVia ‘Somber ON Event’ at the within level, for each person in the model their somber within-person centered scores are regressed on their within-person centered event scores on the same measurement occasion. The regression coefficients and other model parameters – except for the means of somber and event - are the same for all persons.\nVia ‘Event’ we tell Mplus to make Event endogenous like Somber, such that means and variances are estimated for this variable as well - and Event is decomposed in a between person level and within-person level as well. That is, for both Somber and Event we now estimate the mean score over time for each individual inside the model. On the within level the deviations from these means (within-person centered scores) at each time point are modeled. At the between level the relationships among these means, that vary from individual to individual, are modeled.\nVia ‘Somber ON Event’ at the between level, it is specified that the person-specific means for Somber are regressed on the person-specific means for Event.\n\n\n\n\n\n\nWhile the model is running, write down the model in equations.\n\nStart with the decomposition, i.e., \\(E_{it}=\\) …\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[S_{it} = \\mu_{S i} + S_{it}^{(w)}\\] \\[E_{it} = \\mu_{E i} + E_{it}^{(w)}\\]\n\n\n\n\nWrite down the within-level model. Which parameters are estimated at this level?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[S_{it}^{(w)} = b \\times E_{it}^{(w)} + \\zeta_{S it}\\] Parameters estimated at this level:\n\nFixed within-person slope b (same for each person)\nResidual within-person variance for S, the variance of \\(\\zeta_Sit\\)\nWithin-person variance of the (made endogenous) predictor \\(E^{(w)}_{it}\\)\n\n\n\n\n\nWrite down the between level model. Which parameters are estimated at this level?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[\\mu_{S i} = \\gamma_{00} + \\gamma_{01} \\times \\mu_{Ei} + \\zeta_{Si} \\] Parameters estimated at this level:\n\nGrand intercept \\(\\gamma_{00}\\)\nBetween-person slope \\(\\gamma_{01}\\)\nBetween-person residual variance of \\(\\zeta_{Si}\\)\nMean of the between-person predictor \\(\\mu_{Ei}\\)\nVariance of the between-person predictor \\(\\mu_{Ei}\\)\n\n\n\n\n\n\n\nWhen the model is finished running, we first need to check the trace plots of the Bayesian estimation procedure to see if there are signs for non-convergence. Go to the icon with the two graphs, and click on this. Then, choose the option “Bayesian posterior parameter trace plots” and click “View”.\n\nBy clicking “OK” in the next window, the trace plot of parameter 1 appears. You can use the icons with the histograms and the left and right headed arrows to move backward and forward through the parameters (their names are in the headings of the plots).\n\nWrite down how these parameters relate to the ones you identified above (in questions c. and d.),\nAre there any problems with convergence based on these plots? Can you tell what parameter from question b each plot is about?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nParameter 1, %WITHIN%: SOMBER ON EVENT → within-person regression coefficient b\nParameter 2, %WITHIN%: SOMBER → within-person residual variance for somber\nParameter 3, %WITHIN%: EVENT → within-person variance on Event\nParameter 4, %BETWEEN%: [SOMBER] → between-person intercept on Somber (\\(\\gamma_{00}\\))\nParameter 5, %BETWEEN%: [EVENT] → between-person mean on Event\nParameter 6, %BETWEEN%: SOMBER ON EVENT → between-person regression coefficient (\\(\\gamma_{01}\\))\nParameter 7, %BETWEEN%: SOMBER → between-person residual variance of somber\nParameter 8, %BETWEEN%: EVENT → between-person variance on Event\n\nConvergence looks fine for the parameters.\n\n\n\n\n\n\nGo to the output, and consider the parameter estimates. Interpret the findings for the within-person and the between-person slopes (e.g., do they differ from zero, are they positive or negative, what about their size?).\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWithin Level\n\n SOMBER     ON\n    EVENT             -0.203       0.010      0.000      -0.223      -0.184      *\n\n\nBetween Level\n\n SOMBER     ON\n    EVENT             -0.953       0.159      0.000      -1.266      -0.651      *\nBoth slopes are negative, implying that:\n\nWithin person: People’s increased (relative to their mean level) Event scoretends to be followed by a decreased (relative to their mean level) temporary somberness score. A 1 unit higher event score tends to be followed by a decrease of .20 units in their somber score.\nBetween person: People a relatively high (compared to other people) mean for event, tend to have a relatively low (compared to other people) mean for somberness. A 1 unit higher mean would imply about a .95 units lower mean for Somber.\n\nThe between-person slope is more than 4 times steeper: Note however that the between-person variance in Event is about 10 times smaller than the within-person variance in Event (0.278 vs. 2.844). Hence, in the next step we look at the standardized results.\n\n\n\n\n\n\nCheck the standardized results to compare the size of the slopes within and between.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nSOMBER     ON\n    EVENT             -0.259       0.012      0.000      -0.284      -0.236      *\n\nSOMBER     ON\n    EVENT             -0.529       0.074      0.000      -0.656      -0.374      *\nThe standardized between slope is more than twice as steep than the within slope.\nIn terms of \\(R^{2}\\):\n\n6.7% of the momentary within-person variability in Somber is explained by within-person variability in Events.\n28.0% of the stable between-person variability in Somber is predicted by between-person differences in average Event."
  },
  {
    "objectID": "day5_exercises.html#random-intercept-model-with-implicit-lagged-effects-not-dsem",
    "href": "day5_exercises.html#random-intercept-model-with-implicit-lagged-effects-not-dsem",
    "title": "Exercises",
    "section": "",
    "text": "In Model 1 we first decompose each variable at each time point (occasion) in two parts:\n\na person-specific mean, which has the same value at each occasion but differs from person to person\na deviation from that mean for each person and occasion.\n\nHence, we decompose the variables into two latent variables: One with person-specific means, at the between level. One with deviations from these means unique to each occasion and person, at the within level. These deviations can be seen as simply the within-person centered scores for a person at each occasion.\nNext, we add two regression relationships:\n\nBetween level: the person means of Somber are regressed on the person means of Event\nWithin level: the person-mean centered variable Somber is regressed on the person-mean centered variable Event.\n\nNote that this is not a dynamic SEM model yet in terms of the Mplus specification, because there are no explicit lagged relationships in the model. But note that there is an implicit lagged relationship: Although the two variables are measured at the same occasion (as indicated by the subscript t), the variable Event refers to the interval between t-1 and t, while the variable Somber is referring to the specific time point t. Hence, the reported on events take place before the somber feelings.\nThis model can be represented as depicted below, with on the left this decomposition, and on the right the regression models specified at each level.\n\n\n\nTo run this model, use the input file model1.inp. In the ANALYSIS command you will find a number of commands that set up the Bayesian multilevel estimation procedure:\nANALYSIS:   TYPE = TWOLEVEL;\n            ESTIMATOR = BAYES; \n            PROC = 2;\n            BITER= (2000);\n            BSEED = 9556;\nMake sure the following commands for OUTPUT and PLOT are included:\nOUTPUT:     TECH1 STDYX;\nPLOT:       TYPE = PLOT3;\n            FACTORS = ALL;\nCheck out the model specification (i.e., the MODEL command). What do the commands on the within person level and on the between person level do?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nVia ‘Somber ON Event’ at the within level, for each person in the model their somber within-person centered scores are regressed on their within-person centered event scores on the same measurement occasion. The regression coefficients and other model parameters – except for the means of somber and event - are the same for all persons.\nVia ‘Event’ we tell Mplus to make Event endogenous like Somber, such that means and variances are estimated for this variable as well - and Event is decomposed in a between person level and within-person level as well. That is, for both Somber and Event we now estimate the mean score over time for each individual inside the model. On the within level the deviations from these means (within-person centered scores) at each time point are modeled. At the between level the relationships among these means, that vary from individual to individual, are modeled.\nVia ‘Somber ON Event’ at the between level, it is specified that the person-specific means for Somber are regressed on the person-specific means for Event.\n\n\n\n\n\n\nWhile the model is running, write down the model in equations.\n\nStart with the decomposition, i.e., \\(E_{it}=\\) …\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[S_{it} = \\mu_{S i} + S_{it}^{(w)}\\] \\[E_{it} = \\mu_{E i} + E_{it}^{(w)}\\]\n\n\n\n\nWrite down the within-level model. Which parameters are estimated at this level?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[S_{it}^{(w)} = b \\times E_{it}^{(w)} + \\zeta_{S it}\\] Parameters estimated at this level:\n\nFixed within-person slope b (same for each person)\nResidual within-person variance for S, the variance of \\(\\zeta_Sit\\)\nWithin-person variance of the (made endogenous) predictor \\(E^{(w)}_{it}\\)\n\n\n\n\n\nWrite down the between level model. Which parameters are estimated at this level?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[\\mu_{S i} = \\gamma_{00} + \\gamma_{01} \\times \\mu_{Ei} + \\zeta_{Si} \\] Parameters estimated at this level:\n\nGrand intercept \\(\\gamma_{00}\\)\nBetween-person slope \\(\\gamma_{01}\\)\nBetween-person residual variance of \\(\\zeta_{Si}\\)\nMean of the between-person predictor \\(\\mu_{Ei}\\)\nVariance of the between-person predictor \\(\\mu_{Ei}\\)\n\n\n\n\n\n\n\nWhen the model is finished running, we first need to check the trace plots of the Bayesian estimation procedure to see if there are signs for non-convergence. Go to the icon with the two graphs, and click on this. Then, choose the option “Bayesian posterior parameter trace plots” and click “View”.\n\nBy clicking “OK” in the next window, the trace plot of parameter 1 appears. You can use the icons with the histograms and the left and right headed arrows to move backward and forward through the parameters (their names are in the headings of the plots).\n\nWrite down how these parameters relate to the ones you identified above (in questions c. and d.),\nAre there any problems with convergence based on these plots? Can you tell what parameter from question b each plot is about?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nParameter 1, %WITHIN%: SOMBER ON EVENT → within-person regression coefficient b\nParameter 2, %WITHIN%: SOMBER → within-person residual variance for somber\nParameter 3, %WITHIN%: EVENT → within-person variance on Event\nParameter 4, %BETWEEN%: [SOMBER] → between-person intercept on Somber (\\(\\gamma_{00}\\))\nParameter 5, %BETWEEN%: [EVENT] → between-person mean on Event\nParameter 6, %BETWEEN%: SOMBER ON EVENT → between-person regression coefficient (\\(\\gamma_{01}\\))\nParameter 7, %BETWEEN%: SOMBER → between-person residual variance of somber\nParameter 8, %BETWEEN%: EVENT → between-person variance on Event\n\nConvergence looks fine for the parameters.\n\n\n\n\n\n\nGo to the output, and consider the parameter estimates. Interpret the findings for the within-person and the between-person slopes (e.g., do they differ from zero, are they positive or negative, what about their size?).\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWithin Level\n\n SOMBER     ON\n    EVENT             -0.203       0.010      0.000      -0.223      -0.184      *\n\n\nBetween Level\n\n SOMBER     ON\n    EVENT             -0.953       0.159      0.000      -1.266      -0.651      *\nBoth slopes are negative, implying that:\n\nWithin person: People’s increased (relative to their mean level) Event scoretends to be followed by a decreased (relative to their mean level) temporary somberness score. A 1 unit higher event score tends to be followed by a decrease of .20 units in their somber score.\nBetween person: People a relatively high (compared to other people) mean for event, tend to have a relatively low (compared to other people) mean for somberness. A 1 unit higher mean would imply about a .95 units lower mean for Somber.\n\nThe between-person slope is more than 4 times steeper: Note however that the between-person variance in Event is about 10 times smaller than the within-person variance in Event (0.278 vs. 2.844). Hence, in the next step we look at the standardized results.\n\n\n\n\n\n\nCheck the standardized results to compare the size of the slopes within and between.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nSOMBER     ON\n    EVENT             -0.259       0.012      0.000      -0.284      -0.236      *\n\nSOMBER     ON\n    EVENT             -0.529       0.074      0.000      -0.656      -0.374      *\nThe standardized between slope is more than twice as steep than the within slope.\nIn terms of \\(R^{2}\\):\n\n6.7% of the momentary within-person variability in Somber is explained by within-person variability in Events.\n28.0% of the stable between-person variability in Somber is predicted by between-person differences in average Event."
  },
  {
    "objectID": "day5_exercises.html#random-intercept-and-slopes-model-with-implicit-lagged-effects-not-dsem",
    "href": "day5_exercises.html#random-intercept-and-slopes-model-with-implicit-lagged-effects-not-dsem",
    "title": "Exercises",
    "section": "Random Intercept and Slopes Model with Implicit Lagged Effects (not DSEM)",
    "text": "Random Intercept and Slopes Model with Implicit Lagged Effects (not DSEM)\nIn Model 2, we extend Model 1 to allow for individual differences in the within-person slope. This implies that people may respond differently to a temporary increase in the variable Event; this can be interpreted as individual differences in reactivity. This random slope b1 becomes another latent variable at the between-person level, where we can use it as a predictor of the person-specific means of the variable Somber. We will also allow this slope variable to correlate with the means of Event.\nModel 2 can be represented as depicted below, with on the left the within-between decomposition, and on the right the regression models specified at each level. Note the black dot on the arrow to indicate a random slope at the within level.\n\n\nSpecify the Mplus Model\nWrite down the model command for this model. When your are done, check whether you specified it correctly by looking at the provided answers, and/or the input file model2.inp. Note that under command ‘ANALYSIS’ in the input file, we now need to specify type = TWOLEVEL RANDOM.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nMODEL:  %WITHIN%\n              b1 | Somber ON Event;\n              Event;  \n              %BETWEEN%\n              Somber ON Event b1;\n                b1 WITH Event;\n\n\n\n\n\nModel Equations\nWhile the model is running, write down the model equations. The decomposition into within and between will be the same as for Model 1.\n\nWrite down the within level model. What parameters are estimated at this level?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[S^{(w)}_{it}=b_{1i}\\times E^{(w)}_{it} + \\zeta_{S it}\\] The difference is that now the regression coefficient has a subject index i, because it now differs from person to person. The mean and variance of b1i are estimated at the between person level.\nParameters estimated at this level: - Residual within-person variance of \\(\\zeta_{Sit}\\) - Within-person variance of the predictor \\(E^{(w)}_{it}\\)\n\n\n\n\nWrite down the between level model. Which parameters are estimated at this level?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[\\mu_{Si} = \\gamma_{00} + \\gamma_{01} \\times \\mu_{Ei} + \\gamma_{02} \\times b_{1i} + \\zeta_{Si}\\]\nParameters estimated at this level:\n\nGrand intercept \\(\\gamma_{00}\\)\nBetween-person slope \\(\\gamma_{01}\\)\nBetween-person slope \\(\\gamma_{02}\\)\nBetween-person residual variance of \\(\\zeta_{Si}\\)\nMean of the between-person predictor \\(\\mu_{Ei}\\)\nVariance of the between-person predictor \\(\\mu_{Ei}\\)\nMean of the between-person predictor \\(b_{1i}\\) (i.e., within-person slope)\nVariance of the between-person predictor \\(b_{1i}\\)\nCovariance of \\(b_{1i}\\) and \\(\\mu_{Ei}\\) (with statement in mplus code)\n\n\n\n\n\n\nConvergence\nCheck the trace plots of the posteriors for the parameters of the model. Can you link them to the free parameters you identified above? Are there signs of non-convergence?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nAll looks good.\n\n\n\n\n\nInterpret the Results\n\nWhich parameter is the one we should focus on when interested in the average of the person-specific within-person slopes b1?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBetween Level\n\nMeans\n    B1                -0.206       0.014      0.000      -0.234      -0.179      *\nIt is the mean B1 reported at the between level.\n\n\n\n\nThere are two ON statements at the between level now. Report the results for these regressions, and indicate how to interpret these results.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBetween Level\n\nSOMBER     ON\n    B1                -1.966       1.079      0.020      -4.366      -0.095      *\n\n SOMBER     ON\n    EVENT             -0.839       0.176      0.000      -1.188      -0.490      *\nAs before: People with a relatively high (compared to other people) mean for event, tend to have a relatively low (compared to other people) mean for somberness.\nIn addition: People that have a relatively low mean for somber (compared to other people), tend to have a lower within-person slope b1.\n\n\n\n\nInterpreting the effect of b1 on Somber is challenging for most people. What may be helpful is to make a plot of the (bivariate) relationship between b_1i and the within-person mean on somber.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nGo to the plotting options and choose “Between-level scatterplots…”\n\nClick View. Then select “B1, mean” (the posterior means of each person’s B1 coefficient) as the X variable and “SOMBER (estimated cluster mean)” (each person’s mean for Somber) as the Y variable:\n\nClick OK. This produces the scatter plot.\n\n\n\n\nInterpret the relationship you see in the scatterplot.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nThe correlation is negative (about -0.503), indicating that a higher b1 (within-person slope) tends to go together with a lower person-mean on somber.\nNote that the individual differences in b1 run from -0.5 to about 0. Hence, the closer b_1i is to zero, the lower the person’s average score on somber.\nPut differently, people who tend to be less reactive to momentary changes in Events (b_1i closer to zero), are also characterized by a lower trait score on somber.\nNote that this plot is just presenting the bivariate relationship between the two random effects; in contrast, the regression coefficient in the output also is based on a regression model with multiple predictors (in this case, also the person mean on Event). Hence the scatter plot may be helpful, but is not a direct reflection of the regression coefficient (this is the same as in normal multiple linear regression analysis).\n\n\n\n\nLet us look at the person-specific slopes (the random effect b1) that were estimated.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nTo this end, go to the plot menu again and choose the option “Two-level cluster-specific observed and estimated values plots”. Click “View”, and click “OK” in the next window: Now you have a scatter plot of the data of a single individual, with the variable Event on the x-axis and Somber on the between-axis. The slope of the red line is the individual’s b1 (the individual specific regression coefficient for regressing Somber on Event).\nTo fix the axes (which makes it easier to compare the results across individuals, go to the plot menu at the top, choose “Axis properties” and then “Edit settings”:\n\nIn the next window, choose the “Axes Range” tab, click the “Fixed scale” option (at the top), and set the scale range for X from -4 to 4 (because Event was measured on a scale running from -3 to 3), and set the scale of Y from 0 to 8 (because it was measured on a scale from 0 to 7). Then click “OK”.\n\nNow, you can go through the plots of different individuals by clicking the person buttons on the top (with the left arrow to go back, and the right arrow to go forward). Because you fixed the axes, you can easily compare the slopes for different people. This illustrates that every person has his/her own slope."
  },
  {
    "objectID": "day5_exercises.html#dynamic-sem-including-random-autoregressive-effects.",
    "href": "day5_exercises.html#dynamic-sem-including-random-autoregressive-effects.",
    "title": "Exercises",
    "section": "Dynamic SEM: Including random autoregressive effects.",
    "text": "Dynamic SEM: Including random autoregressive effects.\nNext, we include a dynamic relationship in our model. First, create a lagged version of Somber, by using the LAGGED = in the VARIABLE command). In the next step, we’ll include the lagged variable as a predictor at the within level, allowing for a random slope - this produces a random autoregressive effect (each individual will get their own autoregressive effect). This model can be represented as:\n\n\nSpecify the Mplus Model\nWrite out the model command for this model. Check with the input file model3.inp if you specified it correctly.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nMODEL:\n            %WITHIN%\n            s1 | Somber ON Event; ! random regression\n            s2 | Somber ON somber&1; ! random autoregression \n            \n            %BETWEEN%\n            Somber ON Event s1 s2;\nThe command &1 attached to a variable indicates this should be a lag 1 variable. The parameter label followed by | produces a random effect like before.\n\n\n\n\n\nModel Equations\nWhile the model is running, write down the model equations.\n\nWrite down the within level model. What parameters are estimated at this level?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[S^{(w)}_{it}=b_{1i}\\times E^{(w)}_{it} + b_{2i}\\times S^{(w)}_{it-1} + \\zeta_{S it}\\]\nParameters estimated at this level:\n\nResidual within-person variance of \\(\\zeta_{Sit}\\)\nWithin-person variance of the predictor \\(E^{(w)}_{it}\\)\n\n\n\n\n\nWrite down the between level model. Which parameters are estimated at this level?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[\\mu_{Si} = \\gamma_{00} + \\gamma_{01} \\times \\mu_{Ei} + \\gamma_{02} \\times b_{1i} + \\gamma_{03} \\times b_{2i} + \\zeta_{Si}\\]\nParameters estimated at this level:\n\nGrand intercept \\(\\gamma_{00}\\)\nBetween-person slope \\(\\gamma_{01}\\)\nBetween-person slope \\(\\gamma_{02}\\)\nBetween-person slope \\(\\gamma_{03}\\)\nBetween-person residual variance of \\(\\zeta_{Si}\\)\nMean of the between-person predictor \\(\\mu_{Ei}\\)\nVariance of the between-person predictor \\(\\mu_{Ei}\\)\nMean of the between-person predictor \\(b_{1i}\\) (i.e., within-person slope)\nVariance of the between-person predictor \\(b_{1i}\\)\nMean of the between-person predictor \\(b_{2}\\) (i.e., autoregressive slope)\nVariance of the between-person predictor \\(b_{2i}\\)\nCovariances between \\(b_{1i}\\), \\(b_{2i}\\), and \\(\\mu_{Ei}\\)\n\n\n\n\n\n\nConvergence & Interpret the Results\n\nAgain, check the trace plots. If they look okay, consider the parameter estimates. What can you say about the autoregressive effects?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nMeans\n    B2                 0.340       0.022      0.000       0.300       0.383      *\n\nVariances\n    B2                 0.031       0.007      0.000       0.020       0.047      *\nThe average autoregressive effect is 0.34, with SD=0.18 (=sqrt(0.031)). On average, about .34 of the previous Somber score carries over to the next Somber score. The actual autoregressive effects will however differ from person to person.\nHence, on average across persons, about 12% (0.34^2=.12) of the within-person variability in somberness is predicted by the autoregressive effect. The exact numbers will however differ from person to person.\n\n\n\n\nTo obtain some idea of the individual differences in the autoregressive parameter, we will consider diverse plots. One plot that might be of interest is the histogram of the individual parameter values.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nAgain, go to the plotting options, and now select the option: Between-level histogram (etc.)\n\nClick View. In the next window, select the random autoregressive parameter (B2):\n\nClick “OK”. This shows you the sample distribution of the autoregressive parameter. Note that the individual scores are based on the mean of an individual’s posterior distribution for this parameter.\n\n\n\n\nWhat is the highest and the lowest person-specific autoregressive effect?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nThe lowest is about 0.08 (so close to zero), the largest is about 0.7, so strong and positive. Most effects are around .2 to .45.\n\n\n\n\nConsider the level 2 effects where the within-person means of Somber are predicted by the within-person mean on Event, the random slope b1, and by the random slope b2. How would you interpret these results?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBetween Level\n SOMBER     ON\n    B1                -2.342       1.403      0.037      -5.405       0.153\n    B2                 0.181       0.626      0.377      -1.013       1.473\n SOMBER     ON\n    EVENT             -0.831       0.183      0.000      -1.194      -0.476      *\nHence, only the person mean on Event is a significant predictor. The effect is negative, so people with relatively high person-specific means for ‘pleasant Event’ (report higher pleasant Event scores on average compared to others), tend to have relatively low person-specific means for Somber.\nWhile the effect of b1 was significant in a previous model, it no longer is in this model."
  },
  {
    "objectID": "day5_exercises.html#dynamic-sem-including-random-autoregressive-and-cross-lagged-effects",
    "href": "day5_exercises.html#dynamic-sem-including-random-autoregressive-and-cross-lagged-effects",
    "title": "Exercises",
    "section": "Dynamic SEM: Including random autoregressive and cross-lagged effects",
    "text": "Dynamic SEM: Including random autoregressive and cross-lagged effects\nUntil now, we have treated Event as a predictor only. However, it is also possible that the experience of negative and positive Events is affected by a person’s momentary somberness. Moreover, there may be an autoregressive effect for Events. Hence, we are interested in the following model:\n\nThus, we have added two random slopes to the model: b3 and b4, the autoregressive and cross-lagged effect respectively. At the between level, we allow the now 6 random effects to be correlated, rather than that we predict one from the other as we did before. Note that while the figure does not include a relationship between S and E at t-1, these predictors are related in the model (as the model that is specified at time point t, equally applies to all other time points - so we could have added the same b1 arrow there); the current representation simply represents which model statements are needed.\n\nSpecify the Mplus Model\nHow do you specify this model? Check your specification with the model4.inp input file.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nMODEL:\n            %WITHIN%\n            s1 | Somber ON Event; ! lagged random regression\n            s2 | Somber ON somber&1; ! random autoregression for somber\n            s3 | Event ON Event&1; ! random autoregression for Event \n            s4 | Event ON somber&1; ! lagged regression from Somber to Event\n            \n            %BETWEEN%\n            Somber Event s1 s2 s3 s4 WITH Event s1-s4;\n\n\n\n\n\nModel Equations\n\nWhile running the model, write down the regression equation for the within level.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\[S^{(w)}_{it}=b_{1i}\\times E^{(w)}_{it} + b_{2i}\\times S^{(w)}_{it-1} + \\zeta_{S it}\\]\n\\[E^{(w)}_{it}=b_{3i}\\times E^{(w)}_{it-1} + b_{4i}\\times S^{(w)}_{it-1} + \\zeta_{S it}\\]\n\n\n\n\nIn contrast to typical cross-lagged models that we covered in the lecture, we are not including a covariance between the residuals here. Why not?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe concurrent (lag 0) relationship is already accounted for by the (random) regression of \\(S_{it}\\) on \\(E_{it}\\). The residual of \\(S_{it}\\) is the part that cannot be predicted from \\(E_{it}\\) (and \\(S_{it-1}\\)); hence it would not make sense to correlate this residual with the residual of \\(E_{it}\\).\n\n\n\n\nHow many parameters are estimated at each level?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWithin: 2 residual variances\nNote that because Event is now not only a predictor, but also an outcome, we no longer estimate its variance, but instead its residual variance; the variance of Event is now a function of the residual variance, the regression coefficients, and the variances of the predictors.\nBetween:\n\n4 fixed slopes (mean slopes)\n2 fixed intercepts (grand means)\n6 variances of the 4 slopes and 2 intercepts (random effects)\n15 covariances between these random effects\n\n29 parameters in total.\n\n\n\n\n\nConvergence & Interpret the Results\nWhen the model is finished, check the trace plots for signs of non-convergence.\n\nLook at the estimated effects. What can you conclude about the autoregressive and cross-lagged coefficients?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nMeans\n    B1               -0.169       0.014      0.000      -0.195      -0.142      *\n    B2                 0.342       0.022      0.000       0.298       0.382      *\n    B3                 0.118       0.020      0.000       0.078       0.159      *\n    B4               -0.126       0.024      0.000      -0.176      -0.081      *\nAll of the average coefficients differ from zero.\nThe average autoregressive coefficients, b2 and b3, are positive. So for on average, there is carryover from one moment to the next both for feelings of somberness and the experienced pleasantness of events.\nThe average Cross-lagged coefficients b1 and b4 are negative, implying: More positive Events tend to be followed by less somberness an occassion later, while increased somberness tends to by followed by less positively experienced events.\n\n\n\n\nTo compare the strengths of the cross-lagged effects, we need to consider the standardized parameters. Check the standardized output, and interpret the regression coefficients there.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nB1 | SOMBER ON\n    EVENT             -0.211       0.013      0.000      -0.237      -0.187      *\n\n B2 | SOMBER ON\n    SOMBER&1           0.341       0.014      0.000       0.312       0.370      *\n\n B3 | EVENT ON\n    EVENT&1            0.119       0.015      0.000       0.091       0.149      *\n\n B4 | EVENT ON\n    SOMBER&1          -0.105       0.015      0.000      -0.137      -0.077      *\nThe standardized autoregressive parameters should be (approximately) the same as the unstandardized ones. The reason for this is that the variances for \\(y_{t}\\) and \\(y_{t-1}\\) in a stationary model are the same by definition. Slight differences in the point estimates might occur due to the calculation procedure of the standardized effects.\nNote however that the standard errors are smaller for the standardized fixed effects. This is because the standardized fixed effect is a sample mean of the person-specific effects, and disregards sampling variability. Significance of fixed effects should hence be based on the unstandardized fixed effects. This also applies to the fixed cross-lagged effects. This is not an issue for the person-specific standardized effects.\nThe standardized average (fixed) effect from Event to Somber (-.211) is twice the size of the one from Somber to Event (-.105); so on average, the effect from Events on mood is stronger than the other way around. This however may be different from person to person.\n\n\n\n\nConsider the R-square output as well. How can you interpret this?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWithin-Level R-Square Averaged Across Clusters\n                                Posterior  One-Tailed         95% C.I.\n    Variable        Estimate       S.D.      P-Value   Lower 2.5%  Upper 2.5%\n    SOMBER          0.227       0.012      0.000       0.204       0.250\n    EVENT           0.057       0.008      0.000       0.043       0.073\nOn average, 23% of within-person variability in Somber can be accounted for by the lagged relationships; for Event this is 6%. These numbers may however differ from person to person.\n\n\n\n\nConsider the covariances and correlations between the random effects. For which of these correlations is there evidence that they deviate from zero?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThere are a few significant correlations:\nSOMBER   WITH\n    B1                -0.346       0.132      0.007      -0.585      -0.071      *\n    B3                 0.342       0.151      0.017       0.027       0.623      *\n\nB3       WITH\n    B4                 0.571       0.198      0.011       0.102       0.865      *\n\n SOMBER   WITH\n    EVENT             -0.523       0.083      0.000      -0.663      -0.346      *\nPeople with relatively high person-specific means for somber tend to have relatively low values for the effect of event on somber; and relatively high autoregressive effects for Event.\nPeople with relatively higher autoregressive effects for Event tend to have relatively high values for the cross-lagged effect of somber on event.\nPeople with relatively high person-specific means for somber tend to have relatively low person-specific means for event.\nTo interpret these effects further, we’ll look at some plots in the following exercise.\n\n\n\n\nFor each of these correlations, get a scatter plot and interpret the result (use the “Between-level scatterplots” option).\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nSomber with b1: A higher mean level for Somber is related to lower b1 (effect of Event on Somber), which is negative on average; hence people with a lower level on somber, tend to have a b1 closer to zero, while individuals with a higher level on Somber tend to have a b1 that is more negative.\nSomber with b3: Individuals with relatively high person-specific means of Somber tend to have relatively high positive autoregressive effects in their Events.\nb3 with b4: People with relatively higher autoregressive effects for Event (b3) tend to have a relatively high b4 - Since the average b4 is negative, it means: higher autoregression for Event is a b4 closer to zero, while a lower autoregression for Event is associated with a stronger negative effect of Somber on Event.\nPeople with relatively high person-specific means for somber tend to have relatively low person-specific means for event."
  },
  {
    "objectID": "day5_exercises.html#dynamic-sem-adding-an-implicit-lag-2-effect-bonus",
    "href": "day5_exercises.html#dynamic-sem-adding-an-implicit-lag-2-effect-bonus",
    "title": "Exercises",
    "section": "Dynamic SEM: Adding an implicit lag 2 effect (bonus)",
    "text": "Dynamic SEM: Adding an implicit lag 2 effect (bonus)\nAs mentioned before, due to the time interval that the variable Event is referring to, you can think of this as a variable that is situated in time between two consecutive measurements of somber. Hence the lag 0 regression (b1) is in a way already a lagged relationship, and we have therefore not included the regression of Somber at occasion t on Event at occasion t-1, but only a “concurrent” relationship of Somber t on Event t (and hence no residual correlation between the residuals of somber t and event t). However, we could add the lagged effects to the model, which would than imply an implicit ‘lag 2’ effect. We then get the following model:\n\n\nAdditional parameters\nHow many additional parameters does this model have in comparison to the previous one?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe mean (fixed effect) of B5 and its variance (random effect) and the covariance between this random effect and the other 6 random effects; so 8 additional parameters.\n\n\n\n\n\nRun the Mplus Model & Interpret the Results\nRun this model, and interpret the results for the fixed effects of the lagged parameters.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nMeans\n    B1                -0.162       0.014      0.000      -0.190      -0.135      *\n    B2                 0.324       0.022      0.000       0.279       0.367      *\n    B3                 0.116       0.021      0.000       0.076       0.156      *\n    B4                -0.131       0.025      0.000      -0.182      -0.083      *\n    B5                -0.056       0.013      0.000      -0.080      -0.029      *\nThey all differ significantly from zero. The autoregressive parameters (b2 and b3) are positive. The cross-lagged relationships are all negative. The new lagged effect (b5: delayed effect of Event on somber) is also negative; however, it is less strong than the other effect of Event on Somber (b1)."
  },
  {
    "objectID": "day5_exercises.html#dynamic-sem-adding-a-between-level-factor-structure-bonus",
    "href": "day5_exercises.html#dynamic-sem-adding-a-between-level-factor-structure-bonus",
    "title": "Exercises",
    "section": "Dynamic SEM: Adding a between level factor structure (bonus)",
    "text": "Dynamic SEM: Adding a between level factor structure (bonus)\nInstead of simply correlating the random effects at the between level, we may also consider modeling these variables more explicitly. For instance, we can specify a factor model to try to capture what these level 2 variables have in common. This can be represented like this:\n\nHow many parameters does this model have at each level?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWithin: 2 residual variances\nBetween:\n\n7 residual variances for the indicators\n6 factor loadings (one is fixed to 1 for scaling the latent variable)\n1 latent variance for eta (mean of zero)\n7 intercepts (means) for the indicators\n\n\n\n\n\nSpecify the Mplus Model\nTo specify the factor model at the between level, use:\n%BETWEEN%\neta BY somber@1 Event b1*1 b2 b3 b4 b5;\n(The b1*1 is required in the Mplus v8.1; it is probably not necessary any more in a future version.)\n\nWhile running the model, indicate how you would interpret the factor \\(\\eta\\).\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIt is a latent variable which explains the shared variance among the random effects. Without further theory about what kind of latent variable would cause variation in all these random effects it is hard to give it a more meaningful label! Looking at the factor loadings may help interpret the factor here (akin to an EFA).\n\n\n\n\n\nConvergence & Interpret the Results\nIf the model converged, check the parameter estimates. Which factor loadings are significant, and what do they mean?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBetween Level\n\n ETA      BY\n    SOMBER             1.000       0.000      0.000       1.000       1.000\n    EVENT             -0.529       0.257      0.000      -1.365      -0.271      *\n\n ETA      BY\n    B1                -0.062       0.038      0.014      -0.158      -0.006      *\n    B2                -0.016       0.055      0.353      -0.140       0.084\n    B3                 0.095       0.058      0.020       0.005       0.249      *\n    B4                 0.078       0.063      0.069      -0.025       0.240\n    B5                -0.032       0.034      0.159      -0.111       0.031\nPeople with higher eta tend to have:\n\nHigher mean levels of somberness\nLower mean levels of unpleasant-pleasant Event\nMore negative effects of Event t on Somber t (b1) (more reactive to events)\nHigher carry-over in Event (b3) (more intertia in event)\n\nPerhaps the latent variable could be called something like ‘negative outlook’ or ‘neuroticism’. But it would be better to apply a model like this when one has a specific theory in mind!"
  },
  {
    "objectID": "day5_exercises.html#dynamic-sem-making-the-residual-variance-person-specific-bonus",
    "href": "day5_exercises.html#dynamic-sem-making-the-residual-variance-person-specific-bonus",
    "title": "Exercises",
    "section": "Dynamic SEM: Making the residual variance person-specific (bonus)",
    "text": "Dynamic SEM: Making the residual variance person-specific (bonus)\nIn this model, we allow the residual variances to be random. This is often not an option in multilevel software, but within the Bayesian approach of DSEM this is fairly doable. We do still assume normal distributions for the random effects, which is not 100% appropriate for variances as they should not be able to become negative. To avoid issues with this, we model logtransformed random variances instead. The model is depicted below - note that we have also kept the factor structure at the between level from exercise 6:\n\n\nSpecify the Mplus Model\nSpecify this model (or sneak a peak at our input file) and run it. While it is running, determine the number of parameters that are estimated in this model.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWithin level: 0 parameters\nBetween level: - 9 fixed effects - 9 variances - 8 factor loadings - 1 factor variance\n27 parameters in total\n\n\n\n\n\nConvergence\nCheck the trace plots for signs of nonconvergence. Are there signs that indicate convergence may be slow?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nSome of the trace plots show there is a lot of autocorrelation in the Bayesian samples (iterations) over time. This means the chains ‘mix’ slowly - the two chains are not covering each other all the time, and only move slowly along the y-axis.\nHowever, the chains do vary around the same constant (which would be the posterior mean estimate).\nEssentially, there is no evidence that the procedure is not converging, but that it is going relatively slowly. As a result, we’ll need more iterations than if we have lower autocorrelations.\nOne could decide to increase the number of iterations. This can be done by simply increasing the iteration number, or by use the thin option here. When using THIN=10; Mplus saves only every tenth sample of the MCMC chain, and it also means that 10 times as many iterations are used. Thinning reduces (visible) autocorrelation in the chains, and as a result you can more easily see if the chains are mixing well.\n\n\n\n\n\nInterpret the Results\n\nConsider the factor loading estimates: What do you conclude?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBetween Level\n\n ETA      BY\n    SOMBER             1.000       0.000      0.000       1.000       1.000\n    EVENT             -0.371       0.103      0.000      -0.595      -0.187      *\n\n ETA      BY\n    B1                -0.088       0.024      0.000      -0.143      -0.047      *\n    B2                 0.046       0.048      0.153      -0.039       0.147\n    B3                 0.060       0.040      0.051      -0.012       0.144\n    B4                -0.002       0.056      0.489      -0.115       0.105\n    B5                -0.028       0.013      0.014      -0.055      -0.004      *\n    LOGVS              1.412       0.403      0.000       0.856       2.448      *\n    LOGVE              0.057       0.107      0.282      -0.132       0.289\nB3 no longer loads significantly on \\(\\eta\\).\nPeople with relatively high scores on \\(\\eta\\), tend to have:\n\nrelatively high person-specific means for somberness\nrelatively low person-specific means for event\nmore negative effects of \\(Event_{t}\\) on \\(Somber_{t}\\) (b1)\nmore negative delayed effects of \\(Event_{t-1}\\) on \\(Somber_{t}\\) (b5)\nHave a larger (log) residual variance for somberness\n\n\n\n\n\nConsider the R-square in the standardized results: What do you conclude?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWithin: On average across persons, 24% of the within-person variance of momentary somberness and 5% of momentary Events is explained\nBetween: the factor eta accounts for a substantial proportion of between-person variability in average somberness (54%), direct reactivity (b1; 43%), and residual within-person variance in somberness (RVS; 45%).\nIt accounts for a smaller proportion of variance in the average pleasantness of Events (22%) and delayed reactivity (b5; 20%).\nThe other R-squares are closer to zero."
  },
  {
    "objectID": "day5_exercises.html#dsem-adding-a-level-2-between-level-predictor-bonus",
    "href": "day5_exercises.html#dsem-adding-a-level-2-between-level-predictor-bonus",
    "title": "Exercises",
    "section": "DSEM: Adding a level 2 (between level) predictor (bonus)",
    "text": "DSEM: Adding a level 2 (between level) predictor (bonus)\nFinally, we consider a between level observed predictor for the nine random effects. For instance, we may consider a personality trait like Neuroticism as a useful predictor of individual differences in person-specific average somberness, and person-specific average experienced unpleasantness-pleasantness of events. Furthermore, we can investigate whether it predicts individual differences in inertia (autoregression), reactivity (cross-lagged regressions), and residual variances.\n\n\nSpecify the Mplus Model\n\nSpecify the model and run it. While it is running indicate how many parameters this model has.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nWithin level: 0 parameters\nBetween level:\n\n9 fixed effects\n9 variances\n9 regression coefficients\n1 mean for the between level predictor N\n1 variance for the between level predictor N\n\n29 parameters in total (….but see the next exercise)\n\n\n\n\nCheck the number of free parameters in the output (under model fit). Is this correct? If it is not correct, which parameter is added/deleted by default by Mplus?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIt is 30 rather than 29.\nThe additional parameter that is estimates is the covariance between the residuals of the mean Somber and mean Events at the between level.\n\n\n\n\n\nInterpret the results\nInterpret the resulting parameter estimates - focus on the regression coefficients of the model that was estimated.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBetween Level\n\n B1         ON\n    NEUROTIC          -0.004       0.002      0.004      -0.008      -0.001      *\n\nLOGVS      ON\n    NEUROTIC           0.051       0.016      0.001       0.019       0.082      *\n\n SOMBER     ON\n    NEUROTIC           0.056       0.011      0.000       0.035       0.077      *\n\n EVENT      ON\n    NEUROTIC          -0.018       0.007      0.005      -0.032      -0.004      *\nPeople that score relatively high on Neuroticism tend to have relatively:\n\nhigh person-specific means for Somber\nlow person-specific means for Event (on average less pleasant momentary events)\nstronger reactivity to Event (i.e., a more negative b1)\nlarger residual variance for the variable Somber (more unexplained momentary fluctuations for Somberness)"
  },
  {
    "objectID": "day5_introduction.html",
    "href": "day5_introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "In these exercises we focus on analyzing intensive longitudinal data (ILD) obtained with experience sampling (also referred to as ecological momentary assessments).\nWe will use data from Bringmann et al. (2013), stored in file Bringmann1.dat. It contains data from 129 persons, with each between 20 to 60 repeated measurements per variable. Throughout these exercises we will focus on two variables: Somber (to what degree are you feeling somber right now?), and Event (report on the most important event since the previous beep and indicate how unpleasant-pleasant is was).\n\n\n\n\nReferences\n\nBringmann, Laura F., Nathalie Vissers, Marieke Wichers, Nicole Geschwind, Peter Kuppens, Frenk Peeters, Denny Borsboom, and Francis Tuerlinckx. 2013. “A Network Approach to Psychopathology: New Insights into Clinical Longitudinal Data.” Edited by Gabriel Alejandro De Erausquin. PLoS ONE 8 (4): e60188. https://doi.org/10.1371/journal.pone.0060188."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced course on using Mplus",
    "section": "",
    "text": "This course material is part of the Advanced course on using Mplus, a five-day summer school course hosted by Utrecht University’s department of Methodology and Statistics. If you already know how to analyse your data in Mplus but want to learn more about what you are actually doing, and especially if you want to know more about advanced longitudinal analyses, this course is for you. The course consists of in-depth lectures on the fundamentals of SEM, Mplus, and advanced longitudinal models.\nUsing the menu on the left, you can navigate to the exercises of each day."
  },
  {
    "objectID": "index.html#help-and-resources-it-related-issues",
    "href": "index.html#help-and-resources-it-related-issues",
    "title": "Advanced course on using Mplus",
    "section": "Help and Resources: IT-Related Issues",
    "text": "Help and Resources: IT-Related Issues\nBelow we have listed some useful resources in case you run into IT-related issues.\n\n\n\n\n\n\nSolisWorkspace\n\n\n\n\n\nIf you do not have Mplus installed locally on your computer, you will be able to use Mplus for the duration of the summer school via SolisWorkspace (SWS). For this, you need to install both SWS (including the Citrix Receiver app), and set up two-factor authentication (2FA):\nSWS\n\nYou can find the manuals for installing SWS on Windows, MacOS and Linux at https://manuals.uu.nl/en/a-z/?_categorie=applications&_tags=Solisworkspace.\nSolutions to common issues with SWS are described at https://manuals.uu.nl/en/manual/solisworkspace-faq/.\n\n2FA\n\nThe manual for setting up 2FA can be found at https://manuals.uu.nl/en/manual/twee-factor-authenticatie-faq/.\nSolutions to common issues with 2FA are described at https://manuals.uu.nl/en/manual/twee-factor-authenticatie-faq/(https://manuals.uu.nl/en/manual/twee-factor-authenticatie-faq/.\n\nHelp\nIf you need help installing SWS or setting up 2FA, please contact the UU IT Service Desk. They are present on day one of the summer school. Alternatively, you can reach them via\n\nthe IT service desk, to the right of the desk near the Educatorium,\nphone, by calling +31 30 253 4500,\nmail, at servicedesk@uu.nl, or\nor WhatsApp, at +31 6 10 03 44 93.\n\nPlease inform the IT service desk that this concerns an Utrecht Summer School course, and keep your SolisID ready.\nLog-In\nAfter you have installed the software, you can log into SWS using the steps below:\n\nGo to https://solisworkspace.uu.nl.\nLog in using SolisID@soliscom.uu.nl (and replace ‘SolisID’ with the login name you received from the Summerschool).\nConfirm login using 2FA.\n\n\n\n\n\n\n\n\n\n\n\nWiFi\n\n\n\n\n\nThere are two or three options for you to connect to WiFi:\n\nConnect to the Utrecht University-network using your SolisID. As a user name, use SolisID@soliscom.uu.nl, and then fill in the password for the Solis account that you got.\nIf you have Eduroam-access via your own research institute, you can use this also on the Utrecht Science Park.\nUse the Eduroam Visitor Access (EVA), the guest network of Eduroam. For this, you need a ‘day code’, which are depicted on screens in the hallway of UU-buildings. You should text this day code to the phone number also displayed on the screen. Then, you receive a login name and password. You then have WiFi for that entire day. Extra information can be found at https://manuals.uu.nl/a-z/?_tags=eduroam.\n\n\n\n\n\n\n\n\n\n\nPrinter\n\n\n\n\n\nWhen logged in on a UU-computer with a SolisID, you can print by sending the material to the printer and log in with the SolisID on the printer."
  },
  {
    "objectID": "index.html#help-and-resources-sem-analyses",
    "href": "index.html#help-and-resources-sem-analyses",
    "title": "Advanced course on using Mplus",
    "section": "Help and Resources: SEM Analyses",
    "text": "Help and Resources: SEM Analyses\nSome resources about learning more about particular analysis in Mplus are:\n\nThe Mplus User Guide.\nThe Mplus Discussion Forum. Although it has been closed for new questions for some time now, you can still access the hundreds (if not thousands) of questions and answers. You can access the forum here, or simply google your analysis of interest and add “mplus forum” in the search prompt."
  }
]